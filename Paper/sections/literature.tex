\section{Literature Review}
Since \textcite{bardHanabiChallengeNew2020a} introduced the Hanabi Learning Environment (Hanabi-LE) alongside the Hanabi Challenge, the game has become a popular testbed for evaluating multi-agent reinforcement learning algorithms. \textcite{bardHanabiChallengeNew2020a} notes that Hanabi is a challenging environment due to its cooperative nature, limited information, and reliance on communication and reasoning.

Previous efforts for solving Hanabi have used rule-based \cite{coxHowMakePerfect2015,walton-riversEvaluatingModellingHanabiPlaying2017} or imitation learning \cite{osawaSolvingHanabiEstimating2015} based approaches, which have had middling success.
Some notable rule-based approaches include Flawed, IGGI and Piers, as introduced by \textcite{walton-riversEvaluatingModellingHanabiPlaying2017}. Flawed is an agent that is designed to deliberately play sub-optimally with players that are unable to adapt to its behaviour \cite{canaanDiverseAgentsAdHoc2019}. IGGI and Piers follow a similar strategy to each other, where they aim to maximize the expected score of the game by playing the card that is most likely to be playable. The work by \textcite{osawaSolvingHanabiEstimating2015} and \textcite{walton-riversEvaluatingModellingHanabiPlaying2017} both outline the value of recognizing intent and inferring hidden information to achieve high scores in Hanabi by comparing agents that follow fixed rules to those that adapt their strategies based on the actions of other players. Additionally, it has been noted that applying Monte Carlo Tree Search (MCTS) to Hanabi Agents can improve their performance \cite{bardHanabiChallengeNew2020a}. Despite the relative success of rule-based approaches, they all follow strategies that are designed by humans, which makes them intuitive and fairly interpretable, but they rely on established conventions and strategies between players, which inherently limits their ability to adapt to new partners.

Deep reinforcement learning has also been applied to Hanabi due to its ability to be modelled as a decentralized partially observable Markov decision process (Dec-POMDP) \cite{bardHanabiChallengeNew2020a,oliehoekConciseIntroductionDecentralized2016}. The first technique involved applying the Actor-Critic algorithm to Hanabi, which performed well in the two-player version but struggled to learn in larger games \cite{bardHanabiChallengeNew2020a}.

The Rainbow DQN algorithm, introduced by \textcite{hesselRainbowCombiningImprovements2017}, is a deep reinforcement learning algorithm that combines several improvements to the DQN algorithm, such as prioritized experience replay and distributional reinforcement learning. Rainbow DQN has been shown to achieve state-of-the-art performance in various tasks, including playing Atari games and controlling autonomous vehicles. It has shown impressive performance in the two-player self-play version of Hanabi. However, it struggles to play with an unseen partner\cite{canaanEvaluatingRainbowDQN2020}.

Recent work has shown that Rainbow DQN can be extended to learn a more general policy when playing with rule-based agents by training with a diverse population of agents \cite{canaanDiverseAgentsAdHoc2019}. The literature notes that better performance could be achieved with a larger population of diverse agents. Interestingly, the vanilla Rainbow DQN algorithm always converges to the same policy through independent training \cite{canaanDiverseAgentsAdHoc2019,bardHanabiChallengeNew2020a}.

The current state-of-the-art in Hanabi is the Simplified Action Decoder (SAD) algorithm, introduced by \textcite{huSimplifiedActionDecoder2021}. SAD is a deep reinforcement learning augmentation that aims to stabilize the performance of epsilon-greedy exploration. It was developed as an extension of the Bayesian Action decoder(BAD) developed by \textcite
{foersterBayesianActionDecoder2019} and has been shown to achieve superhuman performance in the game when coupled with large-scale RL frameworks like R2D2 \cite{kapturowskiRecurrentExperienceReplay2018}. SAD has been shown to outperform other deep reinforcement learning algorithms, such as Rainbow DQN, in the two-player self-play version of Hanabi \cite{huSimplifiedActionDecoder2021}.

The addition of auxiliary learning tasks has been shown to improve agents' performance in Hanabi by optimising the learning process to focus on the better prediction of unobserved states \cite{huSimplifiedActionDecoder2021}.

Despite the success of SAD, it is not designed to handle the AD-Hoc Teamplay challenge. SAD learns to cooperate effectively with itself or with fixed partners, but it struggles to generalize to new partners \cite{huOtherPlayZeroShotCoordination} due to its tendency to learn arbitrary conventions that are specific to that independent train.

Additionally, agents that rely on Theory of Mind models have been developed. \textcite{fuchsTheoryMindDeep2021} introduced an extension to the Dec-POMDP framework that allows agents to model the beliefs of other agents by providing nested reasoning over their beliefs and other agents' decision-making processes. This Interactive-POMDP (I-POMDP) is effective in learning to cooperate with other agents in Hanabi, with a particular focus on efficient hinting and communication strategies.

Recent work has since shifted away from better, more efficient Reinforcement Learning (RL) algorithms to focus on algorithms that can learn to generalize and adapt to new partners\cite{huOffBeliefLearning2021, huOtherPlayZeroShotCoordination,lucasAnyPlayIntrinsicAugmentation2022,siuEvaluationHumanAITeams2021,zandOntheflyStrategyAdaptation2022}. These algorithms aim to address the Zero-shot Coordination Problem, where agents need to learn to cooperate with new partners without prior exposure. These algorithms do this by augmenting the self-play training process with additional information and exploiting the symmetries of the MDP, such as Any-Play Intrinsic Augmentation (APIA) \cite{lucasAnyPlayIntrinsicAugmentation2022} and Other-Play \cite{huOtherPlayZeroShotCoordination}, or techniques that can change policies during the game, such as On-the-Fly Coordination \cite{zandOntheflyStrategyAdaptation2022}. These techniques allow the agents to better generalize to new partners.

Other approaches to train agents to have more 'human-like' reasoning abilities have been introduced. K-Level-Reasoning \cite{cuiKlevelReasoningZeroShot2021} is a method that utilizes Cognitive Hierarchies, a concept from game theory, to train agents that can reason at multiple levels of reasoning. This is done by training agents to be the best response to agents that reason at a lower level of reasoning. This is similar to Off-Belief Learning \cite{huOffBeliefLearning2021}, which also aims to introduce multi-level cognitive reasoning in a controlled manner. These techniques are effective at training agents that can generalize to new partners.

This, however, requires an incredible amount of computational resources, as the agents need diverse training to learn a general policy on top of the pre-existing sample inefficiency of current RL methods. This is a significant limitation of these algorithms, raising the importance of finding sample-efficient algorithms for Hanabi.