\section{Introduction}
Cooperation is a fundamental aspect of human society. It is the foundation of social structures, economies, and even the evolution of species. Cooperation involves working together in teams to achieve a common goal and sharing resources and information with others. Cooperation is essential for solving complex problems that require the coordination of multiple agents, each with its own goals and objectives. Human beings are naturally adept at cooperating with others, even with challenges such as limited information, communication barriers, and conflicting interests.

Cooperation is a challenging problem in artificial intelligence and has been studied extensively \cite{yuanSurveyProgressCooperative2023}. The ability of agents to cooperate effectively can lead to significant improvements in performance and efficiency. However, cooperation is not always straightforward, especially when agents have limited information about their environment or the other agents they are interacting with. Despite these challenges, significant progress has been made in developing algorithms that enable agents to cooperate in a variety of settings \cite{huSimplifiedActionDecoder2021, liACECooperativeMultiagent2022}.

The game of Hanabi has emerged as a popular benchmark for evaluating the performance of cooperative multi-agent systems. Hanabi is a cooperative card game where players work together to build sets of cards in a specific order. Players have limited information about their own cards and must rely on communication and reasoning to infer their cards from other players' actions. Hanabi's combination of cooperative gameplay, dynamic environments, imperfect information, and reliance on theory of mind reasoning makes it a challenging benchmark for evaluating the performance of computational multi-agents in playing the game \cite{bardHanabiChallengeNew2020a}. Human players have been shown to outperform AI agents by a large margin; even first-time players can easily understand the game and perform well \cite{sidjiHiddenRulesHanabi2023, ArtificialIntelligenceSmart2021}. This makes Hanabi an interesting benchmark for automated game playing. 

A large number of attempts have been made to develop automated players using deep reinforcement learning. Deep reinforcement learning is a sub-field of artificial intelligence that combines deep learning techniques with reinforcement learning to learn complex policies from high-dimensional input data. Deep reinforcement learning algorithms use neural networks to learn a mapping from environmental states to actions. The agent learns to attempt to maximize a reward signal that is provided by the environment. Deep reinforcement learning has been successful in learning to play complex games such as Go \cite{AlphaGoGoogleDeepMind} and StarCraft 2 \cite{googleAlphaStarMasteringRealtime2019}, control autonomous vehicles \cite{bhallaDeepMultiAgent2020}, and solve a variety of sequential decision-making problems \cite{mnihPlayingAtariDeep2013}.

Deep Multi-Agent Reinforcement Learning (MARL) has shown promise in addressing the challenges of cooperation in multi-agent systems. MARL combines deep learning techniques with reinforcement learning in multi-agent settings to enable agents to learn to cooperate effectively in complex and dynamic environments. Recent advances in MARL have led to significant improvements in the performance of agents in cooperative settings, including Hanabi \cite{huOtherPlayZeroShotCoordination, foersterBayesianActionDecoder2019}. However, there are still many open questions and challenges in the field of MARL, particularly in the context of cooperation with new partners, efficient exploration, and sample efficiency \cite{dafoeOpenProblemsCooperative2020, canaanDiverseAgentsAdHoc2019, bardHanabiChallengeNew2020a}.

Deep-Q-learning reinforcement learning techniques, specifically, have shown state-of-the-art performance in Hanabi, with the Rainbow DQN and Simplified Action Decoder agents achieving superhuman performance in self-play \cite{huSimplifiedActionDecoder2021, bardHanabiChallengeNew2020a}. However, their performance in Hanabi has not yet been fully explored. 

This project aims to evaluate the performance of deep-Q-learning techniques in Hanabi's cooperative multi-agent environment. We focus specifically on the Rainbow DQN and Simplified Action Decoder agents. We compare the performance of these agents to simple DQN agents in the two-player setting of Hanabi, focusing on their sample efficiency, exploration and generalization to unseen partners. Our goal is to identify the key factors that contribute to the success of these algorithms in Hanabi and to provide insights into how they can be further improved.

The paper is structured as follows: Section 2 provides a background on Hanabi and the challenges it presents for learning agents, as well as reinforcement learning and deep-Q-learning techniques. Section 3 provides an overview of literature surrounding Hanabi and cooperative MARL. Section 4 describes the methods used in this study, including the experimental setup, agents, training setup, and evaluation metrics. Section 5 presents the results of the experiments and discusses the performance of the agents. Section 6 provides a conclusion and discusses the implications of the findings for future research in cooperative MARL.