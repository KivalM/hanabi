\begin{abstract}
    The card game Hanabi presents a complex, cooperative, multi-agent problem that emphasizes communication and strategy within a partially observable environment. The challenges in developing Hanabi playing agents are similar to many real-world applications requiring efficient cooperation, from self-driving cars to virtual assistants. The importance of understanding how agents can effectively cooperate in such environments has become increasingly important. This paper evaluated the performance of deep-Q-learning techniques, focusing specifically on the Rainbow DQN and Simplified Action Decoder agents, in the cooperative multi-agent environment of Hanabi. The performance of the deep-Q-learning agents are compared in the two-player setting of Hanabi, focusing on their sample efficiency and generalization to unseen partners. Our results show that the Rainbow and the Simplified Action Decoder agents outperform the other agents in the two-player self-play version of Hanabi. Our findings highlight the importance of the Rainbow DQN components in achieving high performance in Hanabi and the importance of the Simplified Action Decoder in stabilizing the learning process and improving the performance of the agents.  Additionally, we find that independently trained agents struggle to cooperate effectively with agents that they have not been trained with. This study contributes insights into the development of more robust and collaborative MARL systems, offering a foundation for future research in similar multi-agent cooperative domains.
\end{abstract}
