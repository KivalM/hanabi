@online{AlphaGoGoogleDeepMind,
  title   = {{{AlphaGo}} - {{Google DeepMind}}},
  url     = {https://deepmind.google/technologies/alphago/},
  urldate = {2024-06-02},
  file    = {/home/skywalker/Zotero/storage/YBS6MMLQ/alphago.html}
}

@online{ArtificialIntelligenceSmart2021,
  title        = {Artificial Intelligence Is Smart, but Does It Play Well with Others?},
  date         = {2021-10-04},
  url          = {https://news.mit.edu/2021/does-artificial-intelligence-play-well-others-1004},
  urldate      = {2024-10-29},
  abstract     = {When Lincoln Laboratory researchers paired humans up with an AI model trained to play a collaborative card game, they found that humans hated playing with the AI teammate when compared to a simpler rule-based agent.},
  langid       = {english},
  organization = {MIT News | Massachusetts Institute of Technology},
  file         = {/home/skywalker/Zotero/storage/2TZYXKZT/does-artificial-intelligence-play-well-others-1004.html}
}

@online{asisMultistepReinforcementLearning2018,
  title       = {Multi-Step {{Reinforcement Learning}}: {{A Unifying Algorithm}}},
  shorttitle  = {Multi-Step {{Reinforcement Learning}}},
  author      = {Asis, Kristopher De and Hernandez-Garcia, J. Fernando and Holland, G. Zacharias and Sutton, Richard S.},
  date        = {2018-06-11},
  eprint      = {1703.01327},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1703.01327},
  urldate     = {2024-10-31},
  abstract    = {Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD(λ) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter λ. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called Q(σ) that unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, σ, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). Q(σ) is generally applicable to both on- and off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of σ, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/IAI4E2P3/Asis et al. - 2018 - Multi-step Reinforcement Learning A Unifying Algorithm.pdf}
}

@online{baconOptionCriticArchitecture2016,
  title      = {The {{Option-Critic Architecture}}},
  author     = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  date       = {2016-12-03},
  eprint     = {1609.05140},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.1609.05140},
  url        = {http://arxiv.org/abs/1609.05140},
  urldate    = {2024-10-25},
  abstract   = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Artificial Intelligence},
  file       = {/home/skywalker/Zotero/storage/BRAWWQNR/Bacon et al. - 2016 - The Option-Critic Architecture.pdf;/home/skywalker/Zotero/storage/QTWDW9J2/1609.html}
}

@article{bardHanabiChallengeNew2020a,
  title        = {The {{Hanabi}} Challenge: {{A}} New Frontier for {{AI}} Research},
  shorttitle   = {The {{Hanabi}} Challenge},
  author       = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},
  date         = {2020-03-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume       = {280},
  pages        = {103216},
  issn         = {0004-3702},
  doi          = {10.1016/j.artint.2019.103216},
  url          = {https://www.sciencedirect.com/science/article/pii/S0004370219300116},
  urldate      = {2024-04-16},
  abstract     = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.},
  keywords     = {Challenge paper,Communication,Cooperative,Games,Imperfect information,Multi-agent learning,Reinforcement learning,Theory of mind},
  file         = {/home/skywalker/Zotero/storage/I788XF54/Bard et al. - 2020 - The Hanabi challenge A new frontier for AI resear.pdf;/home/skywalker/Zotero/storage/Z6Z5RHE8/S0004370219300116.html}
}

@online{beckSurveyMetaReinforcementLearning2024,
  title      = {A {{Survey}} of {{Meta-Reinforcement Learning}}},
  author     = {Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  date       = {2024-08-16},
  eprint     = {2301.08028},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.2301.08028},
  url        = {http://arxiv.org/abs/2301.08028},
  urldate    = {2024-10-30},
  abstract   = {While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/72B4PEKI/Beck et al. - 2024 - A Survey of Meta-Reinforcement Learning.pdf;/home/skywalker/Zotero/storage/27Q4ZTJK/2301.html}
}

@online{bellemareDistributionalPerspectiveReinforcement2017,
  title       = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author      = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  date        = {2017-07-21},
  eprint      = {1707.06887},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1707.06887},
  urldate     = {2024-10-28},
  abstract    = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/WMCIEKDY/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Learning.pdf}
}

@inproceedings{berghAspectsCooperativeCard2017,
  title    = {Aspects of the {{Cooperative Card Game Hanabi}}},
  author   = {Bergh, Mark and Hommelberg, Anne and Kosters, Walter and Spieksma, Flora},
  date     = {2017-09-15},
  pages    = {93--105},
  doi      = {10.1007/978-3-319-67468-1_7},
  abstract = {We examine the cooperative card game Hanabi. Players can only see the cards of the other players, but not their own. Using hints partial information can be revealed. We show some combinatorial properties, and develop AI (Artificial Intelligence) players that use rule-based and Monte Carlo methods.},
  isbn     = {978-3-319-67467-4},
  file     = {/home/skywalker/Zotero/storage/E7CXBSNZ/Bergh et al. - 2017 - Aspects of the Cooperative Card Game Hanabi.pdf}
}

@article{bhallaDeepMultiAgent2020,
  title        = {Deep {{Multi Agent Reinforcement Learning}} for {{Autonomous Driving}}},
  author       = {Bhalla, Sushrut and Subramanian, Sriram Ganapathi and Crowley, Mark},
  date         = {2020},
  journaltitle = {Advances in Artificial Intelligence},
  pages        = {67--78},
  publisher    = {Springer International Publishing},
  url          = {https://cir.nii.ac.jp/crid/1363107370885035008},
  urldate      = {2024-06-02}
}

@online{bouTorchRLDatadrivenDecisionmaking2023,
  title      = {{{TorchRL}}: {{A}} Data-Driven Decision-Making Library for {{PyTorch}}},
  shorttitle = {{{TorchRL}}},
  author     = {Bou, Albert and Bettini, Matteo and Dittert, Sebastian and Kumar, Vikash and Sodhani, Shagun and Yang, Xiaomeng and Fabritiis, Gianni De and Moens, Vincent},
  date       = {2023-11-27},
  eprint     = {2306.00577},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.2306.00577},
  url        = {http://arxiv.org/abs/2306.00577},
  urldate    = {2024-10-29},
  abstract   = {PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control library for PyTorch that provides well-integrated, yet standalone components. We introduce a new and flexible PyTorch primitive, the TensorDict, which facilitates streamlined algorithm development across the many branches of Reinforcement Learning (RL) and control. We provide a detailed description of the building blocks and an extensive overview of the library across domains and tasks. Finally, we experimentally demonstrate its reliability and flexibility and show comparative benchmarks to demonstrate its computational efficiency. TorchRL fosters long-term support and is publicly available on GitHub for greater reproducibility and collaboration within the research community. The code is open-sourced on GitHub.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/RD8KAC9H/Bou et al. - 2023 - TorchRL A data-driven decision-making library for PyTorch.pdf;/home/skywalker/Zotero/storage/QEFRHRQS/2306.html}
}

@article{canaanBehavioralEvaluationHanabi2020,
  title        = {Behavioral {{Evaluation}} of {{Hanabi Rainbow DQN Agents}} and {{Rule-Based Agents}}},
  author       = {Canaan, Rodrigo and Gao, Xianbo and Chung, Youjin and Togelius, Julian and Nealen, Andy and Menzel, Stefan},
  date         = {2020-10-01},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume       = {16},
  number       = {1},
  pages        = {31--37},
  issn         = {2334-0924},
  doi          = {10.1609/aiide.v16i1.7404},
  url          = {https://ojs.aaai.org/index.php/AIIDE/article/view/7404},
  urldate      = {2024-04-16},
  abstract     = {Hanabi is a multiplayer cooperative card game, where only your partners know your cards. All players succeed or fail together. This makes the game an excellent testbed for studying collaboration. Recently, it has been shown that deep neural networks can be trained through self-play to play the game very well. However, such agents generally do not play well with others. In this paper, we investigate the consequences of training Rainbow DQN agents with human-inspired rule-based agents. We analyze with which agents Rainbow agents learn to play well, and how well playing skill transfers to agents they were not trained with. We also analyze patterns of communication between agents to elucidate how collaboration happens. A key finding is that while most agents only learn to play well with partners seen during training, one particular agent leads the Rainbow algorithm towards a much more general policy. The metrics and hypotheses advanced in this paper can be used for further study of collaborative agents.},
  issue        = {1},
  langid       = {english},
  file         = {/home/skywalker/Zotero/storage/64RPGEKP/Canaan et al. - 2020 - Behavioral Evaluation of Hanabi Rainbow DQN Agents.pdf}
}

@inproceedings{canaanDiverseAgentsAdHoc2019,
  title      = {Diverse {{Agents}} for {{Ad-Hoc Cooperation}} in {{Hanabi}}},
  booktitle  = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author     = {Canaan, Rodrigo and Togelius, Julian and Nealen, Andy and Menzel, Stefan},
  date       = {2019-08},
  pages      = {1--8},
  publisher  = {IEEE},
  location   = {London, United Kingdom},
  doi        = {10.1109/CIG.2019.8847944},
  url        = {https://ieeexplore.ieee.org/document/8847944/},
  urldate    = {2024-04-16},
  abstract   = {In this work we present the Hanabi Open Agent Dataset (HOAD)—meant to address the current lack of Hanabi datasets, HOAD is an easily extensible, open-sourced, and comprehensive collection of existing Hanabi playing agents, all ported to the Hanabi Learning Environment (HLE). We give a description and analysis of each agent’s strategy, and we also show cross-play performance between all the agents, demonstrating both their high quality and diversity of strategy. These properties make HOAD especially well suited to studies involving meta-learning and transfer learning. Finally, we describe in detail an easy way to add new agents to HOAD regardless of the origin codebase of the agent and make our code and dataset publicly available at https://github.com/aronsar/hoad.},
  eventtitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  isbn       = {978-1-72811-884-0},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/TWL9TJRM/Canaan et al. - 2019 - Diverse Agents for Ad-Hoc Cooperation in Hanabi.pdf}
}

@online{canaanEvaluatingRainbowDQN2020,
  title       = {Evaluating the {{Rainbow DQN Agent}} in {{Hanabi}} with {{Unseen Partners}}},
  author      = {Canaan, Rodrigo and Gao, Xianbo and Chung, Youjin and Togelius, Julian and Nealen, Andy and Menzel, Stefan},
  date        = {2020-04-28},
  eprint      = {2004.13291},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2004.13291},
  urldate     = {2024-06-24},
  abstract    = {Hanabi is a cooperative game that challenges exist-ing AI techniques due to its focus on modeling the mental states ofother players to interpret and predict their behavior. While thereare agents that can achieve near-perfect scores in the game byagreeing on some shared strategy, comparatively little progresshas been made in ad-hoc cooperation settings, where partnersand strategies are not known in advance. In this paper, we showthat agents trained through self-play using the popular RainbowDQN architecture fail to cooperate well with simple rule-basedagents that were not seen during training and, conversely, whenthese agents are trained to play with any individual rule-basedagent, or even a mix of these agents, they fail to achieve goodself-play scores.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file        = {/home/skywalker/Zotero/storage/G4YFLMKH/Canaan et al. - 2020 - Evaluating the Rainbow DQN Agent in Hanabi with Un.pdf;/home/skywalker/Zotero/storage/UCJDV64S/2004.html}
}

@inproceedings{canaanEvaluatingRlAgents2020,
  title     = {Evaluating Rl Agents in Hanabi with Unseen Partners},
  booktitle = {{{AAAI}}’20 {{Reinforcement Learning}} in {{Games Workshop}}},
  author    = {Canaan, Rodrigo and Gao, Xianbo and Chung, Youjin and Togelius, Julian and Nealen, Andy and Menzel, Stefan},
  date      = {2020},
  url       = {https://www.honda-ri.de/pubs/pdf/4260.pdf},
  urldate   = {2024-06-24},
  file      = {/home/skywalker/Zotero/storage/CY7P53AY/Canaan et al. - 2020 - Evaluating rl agents in hanabi with unseen partner.pdf}
}

@online{CARLASimulator,
  title   = {{{CARLA Simulator}}},
  url     = {https://carla.readthedocs.io/en/latest/},
  urldate = {2024-06-06},
  file    = {/home/skywalker/Zotero/storage/GAR9AKB9/latest.html}
}

@article{coxHowMakePerfect2015,
  title        = {How to {{Make}} the {{Perfect Fireworks Display}}: {{Two Strategies}} for {{Hanabi}}},
  shorttitle   = {How to {{Make}} the {{Perfect Fireworks Display}}},
  author       = {Cox, Christopher and De Silva, Jessica and Deorsey, Philip and Kenter, Franklin H. J. and Retter, Troy and Tobin, Josh},
  date         = {2015-12-01},
  journaltitle = {Mathematics Magazine},
  volume       = {88},
  number       = {5},
  pages        = {323--336},
  publisher    = {Taylor \& Francis},
  issn         = {0025-570X},
  doi          = {10.4169/math.mag.88.5.323},
  url          = {https://doi.org/10.4169/math.mag.88.5.323},
  urldate      = {2024-05-14},
  abstract     = {The game of Hanabi is a multiplayer cooperative card game that has many similarities to a mathematical “hat guessing game.” In Hanabi, a player does not see the cards in her own hand and must rely on the actions of the other players to determine information about her cards. This article presents two strategies for Hanabi. These strategies use different encoding schemes, based on ideas from network coding, to efficiently relay information. The first strategy allows players to effectively recommend moves for other players, and the second strategy allows players to determine the contents of their hands. Results from computer simulations demonstrate that both strategies perform well. In particular, the second strategy achieves a perfect score more than 75 percent of the time.}
}

@inproceedings{cuiKlevelReasoningZeroShot2021,
  title     = {K-Level {{Reasoning}} for {{Zero-Shot Coordination}} in {{Hanabi}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Cui, Brandon and Hu, Hengyuan and Pineda, Luis and Foerster, Jakob},
  date      = {2021},
  volume    = {34},
  pages     = {8215--8228},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/4547dff5fd7604f18c8ee32cf3da41d7-Abstract.html},
  urldate   = {2024-04-16},
  file      = {/home/skywalker/Zotero/storage/38XTYK7L/Cui et al. - 2021 - K-level Reasoning for Zero-Shot Coordination in Ha.pdf}
}

@online{dafoeOpenProblemsCooperative2020,
  title       = {Open {{Problems}} in {{Cooperative AI}}},
  author      = {Dafoe, Allan and Hughes, Edward and Bachrach, Yoram and Collins, Tantum and McKee, Kevin R. and Leibo, Joel Z. and Larson, Kate and Graepel, Thore},
  date        = {2020-12-15},
  eprint      = {2012.08630},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2012.08630},
  url         = {http://arxiv.org/abs/2012.08630},
  urldate     = {2024-06-02},
  abstract    = {Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file        = {/home/skywalker/Zotero/storage/FUAXKQLM/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf;/home/skywalker/Zotero/storage/YEK654ED/2012.html}
}

@online{eysenbachDiversityAllYou2018,
  title       = {Diversity Is {{All You Need}}: {{Learning Skills}} without a {{Reward Function}}},
  shorttitle  = {Diversity Is {{All You Need}}},
  author      = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  date        = {2018-10-09},
  eprint      = {1802.06070},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.1802.06070},
  url         = {http://arxiv.org/abs/1802.06070},
  urldate     = {2024-06-03},
  abstract    = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file        = {/home/skywalker/Zotero/storage/A5RVSFUP/Eysenbach et al. - 2018 - Diversity is All You Need Learning Skills without.pdf;/home/skywalker/Zotero/storage/XZG32TV8/1802.html}
}

@inproceedings{foersterBayesianActionDecoder2019,
  title      = {Bayesian {{Action Decoder}} for {{Deep Multi-Agent Reinforcement Learning}}},
  booktitle  = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author     = {Foerster, Jakob and Song, Francis and Hughes, Edward and Burch, Neil and Dunning, Iain and Whiteson, Shimon and Botvinick, Matthew and Bowling, Michael},
  date       = {2019-05-24},
  pages      = {1942--1951},
  publisher  = {PMLR},
  issn       = {2640-3498},
  url        = {https://proceedings.mlr.press/v97/foerster19a.html},
  urldate    = {2024-04-16},
  abstract   = {When observing the actions of others, humans make inferences about why they acted as they did, and what this implies about the world; humans also use the fact that their actions will be interpreted in this manner, allowing them to act informatively and thereby communicate efficiently with others. Although learning algorithms have recently achieved superhuman performance in a number of two-player, zero-sum games, scalable multi-agent reinforcement learning algorithms that can discover effective strategies and conventions in complex, partially observable settings have proven elusive. We present the Bayesian action decoder (BAD), a new multi-agent learning method that uses an approximate Bayesian update to obtain a public belief that conditions on the actions taken by all agents in the environment. BAD introduces a new Markov decision process, the public belief MDP, in which the action space consists of all deterministic partial policies, and exploits the fact that an agent acting only on this public belief state can still learn to use its private information if the action space is augmented to be over all partial policies mapping private information into environment actions. The Bayesian update is closely related to the theory of mind reasoning that humans carry out when observing others’ actions. We first validate BAD on a proof-of-principle two-step matrix game, where it outperforms policy gradient methods; we then evaluate BAD on the challenging, cooperative partial-information card game Hanabi, where, in the two-player setting, it surpasses all previously published learning and hand-coded approaches, establishing a new state of the art.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/K7HU7YRU/Foerster et al. - 2019 - Bayesian Action Decoder for Deep Multi-Agent Reinf.pdf;/home/skywalker/Zotero/storage/PLCC7BCC/Foerster et al. - 2019 - Bayesian Action Decoder for Deep Multi-Agent Reinf.pdf}
}

@inproceedings{foersterLearningCommunicateDeep2016a,
  title     = {Learning to {{Communicate}} with {{Deep Multi-Agent Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Foerster, Jakob},
  date      = {2016},
  volume    = {29},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html},
  urldate   = {2024-06-02},
  abstract  = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
  file      = {/home/skywalker/Zotero/storage/VLKP4TVI/Foerster et al. - 2016 - Learning to Communicate with Deep Multi-Agent Rein.pdf}
}

@online{fortunatoNoisyNetworksExploration2019,
  title      = {Noisy {{Networks}} for {{Exploration}}},
  author     = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
  date       = {2019-07-09},
  eprint     = {1706.10295},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.1706.10295},
  url        = {http://arxiv.org/abs/1706.10295},
  urldate    = {2024-10-28},
  abstract   = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \$\textbackslash epsilon\$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/ZKKN2SYP/Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf;/home/skywalker/Zotero/storage/24HPPQBZ/1706.html}
}

@online{fuchsTheoryMindDeep2021,
  title       = {Theory of {{Mind}} for {{Deep Reinforcement Learning}} in {{Hanabi}}},
  author      = {Fuchs, Andrew and Walton, Michael and Chadwick, Theresa and Lange, Doug},
  date        = {2021-01-22},
  eprint      = {2101.09328},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2101.09328},
  url         = {http://arxiv.org/abs/2101.09328},
  urldate     = {2024-04-16},
  abstract    = {The partially observable card game Hanabi has recently been proposed as a new AI challenge problem due to its dependence on implicit communication conventions and apparent necessity of theory of mind reasoning for efficient play. In this work, we propose a mechanism for imbuing Reinforcement Learning agents with a theory of mind to discover efficient cooperative strategies in Hanabi. The primary contributions of this work are threefold: First, a formal definition of a computationally tractable mechanism for computing hand probabilities in Hanabi. Second, an extension to conventional Deep Reinforcement Learning that introduces reasoning over finitely nested theory of mind belief hierarchies. Finally, an intrinsic reward mechanism enabled by theory of mind that incentivizes agents to share strategically relevant private knowledge with their teammates. We demonstrate the utility of our algorithm against Rainbow, a state-of-the-art Reinforcement Learning agent.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence},
  file        = {/home/skywalker/Zotero/storage/FLPXQ5QK/Fuchs et al. - 2021 - Theory of Mind for Deep Reinforcement Learning in .pdf;/home/skywalker/Zotero/storage/HIRJMAAE/2101.html}
}

@online{googleAlphaStarMasteringRealtime2019,
  title        = {{{AlphaStar}}: {{Mastering}} the Real-Time Strategy Game {{StarCraft II}}},
  shorttitle   = {{{AlphaStar}}},
  author       = {Google, Deepmind},
  date         = {2019-01-24},
  url          = {https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/},
  urldate      = {2024-06-02},
  abstract     = {Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought...},
  langid       = {english},
  organization = {Google DeepMind},
  file         = {/home/skywalker/Zotero/storage/47CHWMCU/alphastar-mastering-the-real-time-strategy-game-starcraft-ii.html}
}

@software{GoogledeepmindHanabilearningenvironment2024,
  title        = {Google-Deepmind/Hanabi-Learning-Environment},
  date         = {2024-04-19T01:36:45Z},
  origdate     = {2019-01-31T08:42:56Z},
  url          = {https://github.com/google-deepmind/hanabi-learning-environment},
  urldate      = {2024-05-09},
  abstract     = {hanabi\_learning\_environment is a research platform for Hanabi experiments.},
  organization = {Google DeepMind}
}

@article{grootenDeepReinforcementLearning2021,
  title   = {Deep {{Reinforcement Learning}} for the Cooperative Card Game {{Hanabi}}},
  author  = {Grooten, Bram},
  date    = {2021},
  url     = {https://research.tue.nl/files/190273227/Grooten_B.pdf},
  urldate = {2024-06-24},
  file    = {/home/skywalker/Zotero/storage/YKZG8Q9Q/Grooten - 2021 - Deep Reinforcement Learning for the cooperative ca.pdf}
}

@online{grootenVanillaPolicyGradient2022,
  title       = {Is {{Vanilla Policy Gradient Overlooked}}? {{Analyzing Deep Reinforcement Learning}} for {{Hanabi}}},
  shorttitle  = {Is {{Vanilla Policy Gradient Overlooked}}?},
  author      = {Grooten, Bram and Wemmenhove, Jelle and Poot, Maurice and Portegies, Jim},
  date        = {2022-03-22},
  eprint      = {2203.11656},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2203.11656},
  url         = {http://arxiv.org/abs/2203.11656},
  urldate     = {2024-04-16},
  abstract    = {In pursuit of enhanced multi-agent collaboration, we analyze several on-policy deep reinforcement learning algorithms in the recently published Hanabi benchmark. Our research suggests a perhaps counter-intuitive finding, where Proximal Policy Optimization (PPO) is outperformed by Vanilla Policy Gradient over multiple random seeds in a simplified environment of the multi-agent cooperative card game. In our analysis of this behavior we look into Hanabi-specific metrics and hypothesize a reason for PPO's plateau. In addition, we provide proofs for the maximum length of a perfect game (71 turns) and any game (89 turns). Our code can be found at: https://github.com/bramgrooten/DeepRL-for-Hanabi},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file        = {/home/skywalker/Zotero/storage/ID55PWE2/Grooten et al. - 2022 - Is Vanilla Policy Gradient Overlooked Analyzing D.pdf;/home/skywalker/Zotero/storage/AMTDNWLH/2203.html}
}

@online{hafizDeepQNetworkBased2020,
  title      = {Deep {{Q-Network Based Multi-agent Reinforcement Learning}} with {{Binary Action Agents}}},
  author     = {Hafiz, Abdul Mueed and Bhat, Ghulam Mohiuddin},
  date       = {2020-08-06},
  eprint     = {2008.04109},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.2008.04109},
  url        = {http://arxiv.org/abs/2008.04109},
  urldate    = {2024-10-31},
  abstract   = {Deep Q-Network (DQN) based multi-agent systems (MAS) for reinforcement learning (RL) use various schemes where in the agents have to learn and communicate. The learning is however specific to each agent and communication may be satisfactorily designed for the agents. As more complex Deep QNetworks come to the fore, the overall complexity of the multi-agent system increases leading to issues like difficulty in training, need for higher resources and more training time, difficulty in fine-tuning, etc. To address these issues we propose a simple but efficient DQN based MAS for RL which uses shared state and rewards, but agent-specific actions, for updation of the experience replay pool of the DQNs, where each agent is a DQN. The benefits of the approach are overall simplicity, faster convergence and better performance as compared to conventional DQN based approaches. It should be noted that the method can be extended to any DQN. As such we use simple DQN and DDQN (Double Q-learning) respectively on three separate tasks i.e. Cartpole-v1 (OpenAI Gym environment) , LunarLander-v2 (OpenAI Gym environment) and Maze Traversal (customized environment). The proposed approach outperforms the baseline on these tasks by decent margins respectively.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/EXHB2JQM/Hafiz and Bhat - 2020 - Deep Q-Network Based Multi-agent Reinforcement Learning with Binary Action Agents.pdf;/home/skywalker/Zotero/storage/ZQAFU5CL/2008.html}
}

@article{hasseltDeepReinforcementLearning2016a,
  title        = {Deep {{Reinforcement Learning}} with {{Double Q-Learning}}},
  author       = {van Hasselt,Hado and Guez, Arthur and Silver, David},
  date         = {2016-03-02},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume       = {30},
  number       = {1},
  issn         = {2374-3468},
  doi          = {10.1609/aaai.v30i1.10295},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/10295},
  urldate      = {2024-10-28},
  abstract     = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented.  In this paper, we answer all these questions affirmatively.  In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain.  We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation.  We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  issue        = {1},
  langid       = {english},
  file         = {/home/skywalker/Zotero/storage/4BQIKCGU/Hasselt et al. - 2016 - Deep Reinforcement Learning with Double Q-Learning.pdf}
}

@online{hesselRainbowCombiningImprovements2017,
  title       = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle  = {Rainbow},
  author      = {Hessel, Matteo and Modayil, Joseph and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  date        = {2017-10-06},
  eprint      = {1710.02298},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.1710.02298},
  url         = {http://arxiv.org/abs/1710.02298},
  urldate     = {2024-05-16},
  abstract    = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/EKCQYFZ6/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;/home/skywalker/Zotero/storage/KC9BYBRJ/1710.html}
}

@online{huOffBeliefLearning2021,
  title       = {Off-{{Belief Learning}}},
  author      = {Hu, Hengyuan and Lerer, Adam and Cui, Brandon and Wu, David and Pineda, Luis and Brown, Noam and Foerster, Jakob},
  date        = {2021-08-17},
  eprint      = {2103.04000},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2103.04000},
  url         = {http://arxiv.org/abs/2103.04000},
  urldate     = {2024-05-16},
  abstract    = {The standard problem setting in Dec-POMDPs is self-play, where the goal is to find a set of policies that play optimally together. Policies learned through self-play may adopt arbitrary conventions and implicitly rely on multi-step reasoning based on fragile assumptions about other agents' actions and thus fail when paired with humans or independently trained agents at test time. To address this, we present off-belief learning (OBL). At each timestep OBL agents follow a policy \$\textbackslash pi\_1\$ that is optimized assuming past actions were taken by a given, fixed policy (\$\textbackslash pi\_0\$), but assuming that future actions will be taken by \$\textbackslash pi\_1\$. When \$\textbackslash pi\_0\$ is uniform random, OBL converges to an optimal policy that does not rely on inferences based on other agents' behavior (an optimal grounded policy). OBL can be iterated in a hierarchy, where the optimal policy from one level becomes the input to the next, thereby introducing multi-level cognitive reasoning in a controlled manner. Unlike existing approaches, which may converge to any equilibrium policy, OBL converges to a unique policy, making it suitable for zero-shot coordination (ZSC). OBL can be scaled to high-dimensional settings with a fictitious transition mechanism and shows strong performance in both a toy-setting and the benchmark human-AI \& ZSC problem Hanabi.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/J4F5VIZZ/Hu et al. - 2021 - Off-Belief Learning.pdf;/home/skywalker/Zotero/storage/8XGC5TKR/2103.html}
}

@inproceedings{huOtherPlayZeroShotCoordination,
  title     = {“{{Other-Play}}” for {{Zero-Shot Coordination}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author    = {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  date      = {2020},
  pages     = {4399--4410},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/v119/hu20a.html},
  urldate   = {2024-06-24}
}

@online{huSimplifiedActionDecoder2021,
  title       = {Simplified {{Action Decoder}} for {{Deep Multi-Agent Reinforcement Learning}}},
  author      = {Hu, Hengyuan and Foerster, Jakob N.},
  date        = {2021-05-12},
  eprint      = {1912.02288},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1912.02288},
  urldate     = {2024-04-16},
  abstract    = {In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e., the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with best practices for multi-agent learning, SAD establishes a new SOTA for learning methods for 2-5 players on the self-play part of the Hanabi challenge. Our ablations show the contributions of SAD compared with the best practice components. All of our code and trained agents are available at https://github.com/facebookresearch/Hanabi\_SAD.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence},
  file        = {/home/skywalker/Zotero/storage/ABJA9LY4/Hu and Foerster - 2021 - Simplified Action Decoder for Deep Multi-Agent Rei.pdf}
}

@article{kangBayesianReinforcementLearning2024,
  title        = {Bayesian Reinforcement Learning: {{A}} Basic Overview},
  shorttitle   = {Bayesian Reinforcement Learning},
  author       = {Kang, Pyungwon and Tobler, Philippe N. and Dayan, Peter},
  date         = {2024-05-01},
  journaltitle = {Neurobiology of Learning and Memory},
  shortjournal = {Neurobiology of Learning and Memory},
  volume       = {211},
  pages        = {107924},
  issn         = {1074-7427},
  doi          = {10.1016/j.nlm.2024.107924},
  url          = {https://www.sciencedirect.com/science/article/pii/S1074742724000352},
  urldate      = {2024-08-18},
  abstract     = {We and other animals learn because there is some aspect of the world about which we are uncertain. This uncertainty arises from initial ignorance, and from changes in the world that we do not perfectly know; the uncertainty often becomes evident when our predictions about the world are found to be erroneous. The Rescorla-Wagner learning rule, which specifies one way that prediction errors can occasion learning, has been hugely influential as a characterization of Pavlovian conditioning and, through its equivalence to the delta rule in engineering, in a much wider class of learning problems. Here, we review the embedding of the Rescorla-Wagner rule in a Bayesian context that is precise about the link between uncertainty and learning, and thereby discuss extensions to such suggestions as the Kalman filter, structure learning, and beyond, that collectively encompass a wider range of uncertainties and accommodate a wider assortment of phenomena in conditioning.},
  keywords     = {Bayesian approach,Reinforcement learning},
  file         = {/home/skywalker/Zotero/storage/RL5VJTAQ/S1074742724000352.html}
}

@inproceedings{kapturowskiRecurrentExperienceReplay2018,
  title      = {Recurrent {{Experience Replay}} in {{Distributed Reinforcement Learning}}},
  author     = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  date       = {2018-09-27},
  url        = {https://openreview.net/forum?id=r1lyTjAqYX},
  urldate    = {2024-10-29},
  abstract   = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/BEHHV5S6/Kapturowski et al. - 2018 - Recurrent Experience Replay in Distributed Reinforcement Learning.pdf}
}

@online{kiranDeepReinforcementLearning2021,
  title      = {Deep {{Reinforcement Learning}} for {{Autonomous Driving}}: {{A Survey}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Autonomous Driving}}},
  author     = {Kiran, B. Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
  date       = {2021-01-23},
  eprint     = {2002.00444},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.2002.00444},
  url        = {http://arxiv.org/abs/2002.00444},
  urldate    = {2024-10-28},
  abstract   = {With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file       = {/home/skywalker/Zotero/storage/IBYJDZAY/Kiran et al. - 2021 - Deep Reinforcement Learning for Autonomous Driving A Survey.pdf;/home/skywalker/Zotero/storage/B7H7TLT7/2002.html}
}

@online{liACECooperativeMultiagent2022,
  title       = {{{ACE}}: {{Cooperative Multi-agent Q-learning}} with {{Bidirectional Action-Dependency}}},
  shorttitle  = {{{ACE}}},
  author      = {Li, Chuming and Liu, Jie and Zhang, Yinmin and Wei, Yuhong and Niu, Yazhe and Yang, Yaodong and Liu, Yu and Ouyang, Wanli},
  date        = {2022-12-02},
  eprint      = {2211.16068},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2211.16068},
  url         = {http://arxiv.org/abs/2211.16068},
  urldate     = {2024-06-02},
  abstract    = {Multi-agent reinforcement learning (MARL) suffers from the non-stationarity problem, which is the ever-changing targets at every iteration when multiple agents update their policies at the same time. Starting from first principle, in this paper, we manage to solve the non-stationarity problem by proposing bidirectional action-dependent Q-learning (ACE). Central to the development of ACE is the sequential decision-making process wherein only one agent is allowed to take action at one time. Within this process, each agent maximizes its value function given the actions taken by the preceding agents at the inference stage. In the learning phase, each agent minimizes the TD error that is dependent on how the subsequent agents have reacted to their chosen action. Given the design of bidirectional dependency, ACE effectively turns a multiagent MDP into a single-agent MDP. We implement the ACE framework by identifying the proper network representation to formulate the action dependency, so that the sequential decision process is computed implicitly in one forward pass. To validate ACE, we compare it with strong baselines on two MARL benchmarks. Empirical experiments demonstrate that ACE outperforms the state-of-the-art algorithms on Google Research Football and StarCraft Multi-Agent Challenge by a large margin. In particular, on SMAC tasks, ACE achieves 100\% success rate on almost all the hard and super-hard maps. We further study extensive research problems regarding ACE, including extension, generalization, and practicability. Code is made available to facilitate further research.},
  pubstate    = {prepublished},
  version     = {2},
  keywords    = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file        = {/home/skywalker/Zotero/storage/CM36LFJF/Li et al. - 2022 - ACE Cooperative Multi-agent Q-learning with Bidir.pdf;/home/skywalker/Zotero/storage/JT7QHAJF/2211.html}
}

@online{lillicrapContinuousControlDeep2019,
  title      = {Continuous Control with Deep Reinforcement Learning},
  author     = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date       = {2019-07-05},
  eprint     = {1509.02971},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.1509.02971},
  url        = {http://arxiv.org/abs/1509.02971},
  urldate    = {2024-10-25},
  abstract   = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  pubstate   = {prepublished},
  version    = {6},
  keywords   = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/YZK8SZ6K/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learning.pdf;/home/skywalker/Zotero/storage/U5Q34RYQ/1509.html}
}

@inproceedings{loweMultiAgentActorCriticMixed2017,
  title     = {Multi-{{Agent Actor-Critic}} for {{Mixed Cooperative-Competitive Environments}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Lowe, Ryan and family=WU, given=YI, given-i=YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  date      = {2017},
  volume    = {30},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html},
  urldate   = {2024-06-02},
  abstract  = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  file      = {/home/skywalker/Zotero/storage/NN987MPW/Lowe et al. - 2017 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf}
}

@online{lucasAnyPlayIntrinsicAugmentation2022,
  title       = {Any-{{Play}}: {{An Intrinsic Augmentation}} for {{Zero-Shot Coordination}}},
  shorttitle  = {Any-{{Play}}},
  author      = {Lucas, Keane and Allen, Ross E.},
  date        = {2022-01-28},
  eprint      = {2201.12436},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2201.12436},
  url         = {http://arxiv.org/abs/2201.12436},
  urldate     = {2024-04-17},
  abstract    = {Cooperative artificial intelligence with human or superhuman proficiency in collaborative tasks stands at the frontier of machine learning research. Prior work has tended to evaluate cooperative AI performance under the restrictive paradigms of self-play (teams composed of agents trained together) and cross-play (teams of agents trained independently but using the same algorithm). Recent work has indicated that AI optimized for these narrow settings may make for undesirable collaborators in the real-world. We formalize an alternative criteria for evaluating cooperative AI, referred to as inter-algorithm cross-play, where agents are evaluated on teaming performance with all other agents within an experiment pool with no assumption of algorithmic similarities between agents. We show that existing state-of-the-art cooperative AI algorithms, such as Other-Play and Off-Belief Learning, under-perform in this paradigm. We propose the Any-Play learning augmentation -- a multi-agent extension of diversity-based intrinsic rewards for zero-shot coordination (ZSC) -- for generalizing self-play-based algorithms to the inter-algorithm cross-play setting. We apply the Any-Play learning augmentation to the Simplified Action Decoder (SAD) and demonstrate state-of-the-art performance in the collaborative card game Hanabi.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,I.2.11},
  file        = {/home/skywalker/Zotero/storage/SPTN7U5L/Lucas and Allen - 2022 - Any-Play An Intrinsic Augmentation for Zero-Shot .pdf;/home/skywalker/Zotero/storage/VZ8Q8XXS/2201.html}
}

@inproceedings{lupuTrajectoryDiversityZeroShot2021,
  title      = {Trajectory {{Diversity}} for {{Zero-Shot Coordination}}},
  booktitle  = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author     = {Lupu, Andrei and Cui, Brandon and Hu, Hengyuan and Foerster, Jakob},
  date       = {2021-07-01},
  pages      = {7204--7213},
  publisher  = {PMLR},
  issn       = {2640-3498},
  url        = {https://proceedings.mlr.press/v139/lupu21a.html},
  urldate    = {2024-04-17},
  abstract   = {We study the problem of zero-shot coordination (ZSC), where agents must independently produce strategies for a collaborative game that are compatible with novel partners not seen during training. Our first contribution is to consider the need for diversity in generating such agents. Because self-play (SP) agents control their own trajectory distribution during training, each policy typically only performs well on this exact distribution. As a result, they achieve low scores in ZSC, since playing with another agent is likely to put them in situations they have not encountered during training. To address this issue, we train a common best response (BR) to a population of agents, which we regulate to be diverse. To this end, we introduce \textbackslash textit\{Trajectory Diversity\} (TrajeDi) – a differentiable objective for generating diverse reinforcement learning policies. We derive TrajeDi as a generalization of the Jensen-Shannon divergence between policies and motivate it experimentally in two simple settings. We then focus on the collaborative card game Hanabi, demonstrating the scalability of our method and improving upon the cross-play scores of both independently trained SP agents and BRs to unregularized populations.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/QC2PZ3HU/Lupu et al. - 2021 - Trajectory Diversity for Zero-Shot Coordination.pdf;/home/skywalker/Zotero/storage/XXUJRM94/Lupu et al. - 2021 - Trajectory Diversity for Zero-Shot Coordination.pdf}
}

@online{mnihPlayingAtariDeep2013,
  title      = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author     = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date       = {2013-12-19},
  eprint     = {1312.5602},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.1312.5602},
  url        = {http://arxiv.org/abs/1312.5602},
  urldate    = {2024-10-24},
  abstract   = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/B2NBDQGY/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/skywalker/Zotero/storage/NXM7T5AV/1312.html}
}

@online{MultiAgentReinforcementLearning,
  title      = {Multi-{{Agent Reinforcement Learning}}: {{Foundations}} and {{Modern Approaches}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  url        = {https://www.marl-book.com/},
  urldate    = {2024-04-11},
  abstract   = {Textbook published by MIT Press},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/S4FCIYUN/www.marl-book.com.html}
}

@inproceedings{nekoeiFewshotCoordinationRevisiting2023,
  title      = {Towards {{Few-shot Coordination}}: {{Revisiting Ad-hoc Teamplay Challenge In}} the {{Game}} of {{Hanabi}}},
  shorttitle = {Towards {{Few-shot Coordination}}},
  booktitle  = {Proceedings of {{The}} 2nd {{Conference}} on {{Lifelong Learning Agents}}},
  author     = {Nekoei, Hadi and Zhao, Xutong and Rajendran, Janarthanan and Liu, Miao and Chandar, Sarath},
  date       = {2023-11-20},
  pages      = {861--877},
  publisher  = {PMLR},
  issn       = {2640-3498},
  url        = {https://proceedings.mlr.press/v232/nekoei23b.html},
  urldate    = {2024-04-16},
  abstract   = {Cooperative Multi-agent Reinforcement Learning (MARL) algorithms with Zero-Shot Coordination (ZSC) have gained significant attention in recent years. ZSC refers to the ability of agents to coordinate with independently trained agents. While ZSC is crucial for cooperative MARL agents, it might not be possible for complex tasks and changing environments. Agents also need to adapt and improve their performance with minimal interaction with other agents. In this work, we show empirically that state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different methods, and they require millions of samples to adapt to these new partners. To investigate this issue, we formally defined a framework based on a popular cooperative multi-agent game called Hanabi to evaluate the adaptability of MARL methods. In particular, we created a diverse set of pre-trained agents and defined a new metric called adaptation regret that measures the agent’s ability to efficiently adapt and improve its coordination performance when paired with some held-out pool of partners on top of its ZSC performance. After evaluating several SOTA algorithms using our framework, our experiments reveal that naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL). This finding raises an interesting research question: How to design MARL algorithms with high ZSC performance and capability of fast adaptation to unseen partners. As a first step, we studied the role of different hyper-parameters and design choices on the adaptability of current MARL algorithms. Our experiments show that two categories of hyper-parameters controlling the data diversity and optimization process have a significant impact on the adaptability of Hanabi agents. We hope this initial analysis will inspire more work on designing both general and adaptive MARL algorithms.},
  eventtitle = {Conference on {{Lifelong Learning Agents}}},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/KH5THJFR/Nekoei et al. - 2023 - Towards Few-shot Coordination Revisiting Ad-hoc T.pdf}
}

@article{ningSurveyMultiagentReinforcement2024,
  title        = {A Survey on Multi-Agent Reinforcement Learning and Its Application},
  author       = {Ning, Zepeng and Xie, Lihua},
  date         = {2024-02-18},
  journaltitle = {Journal of Automation and Intelligence},
  shortjournal = {Journal of Automation and Intelligence},
  issn         = {2949-8554},
  doi          = {10.1016/j.jai.2024.02.003},
  url          = {https://www.sciencedirect.com/science/article/pii/S2949855424000042},
  urldate      = {2024-04-17},
  abstract     = {Multi-agent reinforcement learning (MARL) has been a rapidly evolving field. This paper presents a comprehensive survey of MARL and its applications. We trace the historical evolution of MARL, highlight its progress, and discuss related survey works. Then, we review the existing works addressing inherent challenges and those focusing on diverse applications. Some representative stochastic games, MARL means, spatial forms of MARL, and task classification are revisited. We then conduct an in-depth exploration of a variety of challenges encountered in MARL applications. We also address critical operational aspects, such as hyperparameter tuning and computational complexity, which are pivotal in practical implementations of MARL. Afterward, we make a thorough overview of the applications of MARL to intelligent machines and devices, chemical engineering, biotechnology, healthcare, and societal issues, which highlights the extensive potential and relevance of MARL within both current and future technological contexts. Our survey also encompasses a detailed examination of benchmark environments used in MARL research, which are instrumental in evaluating MARL algorithms and demonstrate the adaptability of MARL to diverse application scenarios. In the end, we give our prospect for MARL and discuss their related techniques and potential future applications.},
  keywords     = {Benchmark environments,Multi-agent reinforcement learning,Multi-agent systems,Stochastic games},
  file         = {/home/skywalker/Zotero/storage/W7IGC8YQ/S2949855424000042.html}
}

@book{oliehoekConciseIntroductionDecentralized2016,
  title     = {A {{Concise Introduction}} to {{Decentralized POMDPs}}},
  author    = {Oliehoek, Frans A. and Amato, Christopher},
  date      = {2016-05},
  edition   = {1},
  publisher = {Springer Publishing Company, Incorporated},
  abstract  = {This book introduces multiagent planning under uncertainty as formalized by decentralized partially observable Markov decision processes (Dec-POMDPs). The intended audience is researchers and graduate students working in the fields of artificial intelligence related to sequential decision making: reinforcement learning, decision-theoretic planning for single agents, classical multiagent planning, decentralized control, and operations research.},
  isbn      = {978-3-319-28927-4},
  pagetotal = {134}
}

@online{oroojlooyjadidReviewCooperativeMultiAgent2021,
  title       = {A {{Review}} of {{Cooperative Multi-Agent Deep Reinforcement Learning}}},
  author      = {OroojlooyJadid, Afshin and Hajinezhad, Davood},
  date        = {2021-04-30},
  eprint      = {1908.03963},
  eprinttype  = {arXiv},
  eprintclass = {cs, math, stat},
  url         = {http://arxiv.org/abs/1908.03963},
  urldate     = {2024-05-09},
  abstract    = {Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. In this review article, we have focused on presenting recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. In particular, we have focused on five common approaches on modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critic, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. First, we elaborate on each of these methods, possible challenges, and how these challenges were mitigated in the relevant papers. If applicable, we further make a connection among different papers in each category. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. Due to the recent success of MARL in real-world applications, we assign a section to provide a review of these applications and corresponding articles. Also, a list of available environments for MARL research is provided in this survey. Finally, the paper is concluded with proposals on the possible research directions.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/8KEXDLFI/OroojlooyJadid and Hajinezhad - 2021 - A Review of Cooperative Multi-Agent Deep Reinforce.pdf}
}

@article{oroojlooyReviewCooperativeMultiagent2022,
  title        = {A Review of Cooperative Multi-Agent Deep Reinforcement Learning},
  author       = {Oroojlooy, Afshin and Hajinezhad, Davood},
  date         = {2022-10-14},
  journaltitle = {Applied Intelligence},
  shortjournal = {Applied Intelligence},
  volume       = {53},
  number       = {11},
  pages        = {13677--13722},
  issn         = {0924-669X},
  doi          = {10.1007/s10489-022-04105-y},
  url          = {https://doi.org/10.1007/s10489-022-04105-y},
  urldate      = {2024-06-02},
  abstract     = {Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. The aim of this review article is to provide an overview of recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. Our classification of MARL approaches includes five categories for modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critics, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. We first discuss each of these methods, their potential challenges, and how these challenges were mitigated in the relevant papers. Additionally, we make connections among different papers in each category if applicable. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. In light of MARL’s recent success in real-world applications, we have dedicated a section to reviewing these applications and articles. This survey also provides a list of available environments for MARL research. Finally, the paper is concluded with proposals on possible research directions.},
  keywords     = {Cooperative learning,Multi-agent systems,Reinforcement learning},
  file         = {/home/skywalker/Zotero/storage/JB9UHDCX/Oroojlooy and Hajinezhad - 2022 - A review of cooperative multi-agent deep reinforce.pdf}
}

@inproceedings{osawaSolvingHanabiEstimating2015,
  title      = {Solving Hanabi: {{Estimating}} Hands by Opponent's Actions in Cooperative Game with Incomplete Information},
  shorttitle = {Solving Hanabi},
  booktitle  = {Computer {{Poker}} and {{Imperfect Information}} - {{Papers Presented}} at the 29th {{AAAI Conference}} on {{Artificial Intelligence}}, {{Technical Report}}},
  author     = {Osawa, Hirotaka},
  date       = {2015},
  pages      = {37--43},
  publisher  = {AI Access Foundation},
  url        = {https://keio.elsevierpure.com/en/publications/solving-hanabi-estimating-hands-by-opponents-actions-in-cooperati},
  urldate    = {2024-05-14},
  eventtitle = {29th {{AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2015},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/4RSY2AZJ/solving-hanabi-estimating-hands-by-opponents-actions-in-cooperati.html}
}

@online{PapersCodeR2D2,
  title    = {Papers with {{Code}} - {{R2D2 Explained}}},
  url      = {https://paperswithcode.com/method/r2d2},
  urldate  = {2024-10-29},
  abstract = {Building on the recent successes of distributed training of RL agents, R2D2 is an RL approach that trains a RNN-based RL agents from distributed prioritized experience replay.  Using a single network architecture and fixed set of hyperparameters, Recurrent Replay Distributed DQN quadrupled the previous state of the art on Atari-57, and matches the state of the art on DMLab-30.  It was the first agent to exceed human-level performance in 52 of the 57 Atari games.},
  langid   = {english},
  file     = {/home/skywalker/Zotero/storage/YLMPEXB7/r2d2.html}
}

@online{PettingZooDocumentation,
  title   = {{{PettingZoo Documentation}}},
  url     = {https://pettingzoo.farama.org/index.html},
  urldate = {2024-10-29},
  langid  = {english},
  file    = {/home/skywalker/Zotero/storage/KH4YEIG4/pettingzoo.farama.org.html}
}

@incollection{putermanChapterMarkovDecision1990,
  title     = {Chapter 8 {{Markov}} Decision Processes},
  booktitle = {Handbooks in {{Operations Research}} and {{Management Science}}},
  author    = {Puterman, Martin L.},
  date      = {1990-01-01},
  series    = {Stochastic {{Models}}},
  volume    = {2},
  pages     = {331--434},
  publisher = {Elsevier},
  doi       = {10.1016/S0927-0507(05)80172-0},
  url       = {https://www.sciencedirect.com/science/article/pii/S0927050705801720},
  urldate   = {2024-06-24},
  abstract  = {This chapter presents theory, applications, and computational methods for Markov Decision Processes (MDP's). MDP's are a class of stochastic sequential decision processes in which the cost and transition functions depend only on the current state of the system and the current action. These models have been applied in a wide range of subject areas, most notably in queueing and inventory control. A sequential decision process is a model for dynamic system under the control of a decision maker. Sequential decision processes are classified according to the times (epochs) at which decisions are made, the length of the decision making horizon, the mathematical properties of the state and action spaces, and the optimality criteria. The focus of this chapter is problems in which decisions are made periodically at discrete time points. The state and action sets are either finite, countable, compact or Borel; their characteristics determine the form of the reward and transition probability functions. The optimality criteria considered in the chapter include finite and infinite horizon expected total reward, infinite horizon expected total discounted reward, and average expected reward. The main objectives in analyzing sequential decision processes in general and MDP's in particular include (1) providing an optimality equation that characterizes the supremal value of the objective function, (2) characterizing the form of an optimal policy if it exists, (3) developing efficient computational procedures for finding policies thatare optimal or close to optimal. The optimality or Bellman equation is the basic entity in MDP theory and almost all existence, characterization, and computational results are based on its analysis.},
  file      = {/home/skywalker/Zotero/storage/FFTMJM9A/S0927050705801720.html}
}

@online{samvelyanStarCraftMultiAgentChallenge2019,
  title       = {The {{StarCraft Multi-Agent Challenge}}},
  author      = {Samvelyan, Mikayel and Rashid, Tabish and family=Witt, given=Christian Schroeder, prefix=de, useprefix=true and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
  date        = {2019-12-09},
  eprint      = {1902.04043},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  doi         = {10.48550/arXiv.1902.04043},
  url         = {http://arxiv.org/abs/1902.04043},
  urldate     = {2024-04-17},
  abstract    = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ\_obZ0.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/QC2MIIZZ/Samvelyan et al. - 2019 - The StarCraft Multi-Agent Challenge.pdf;/home/skywalker/Zotero/storage/QF7HNP2S/1902.html}
}

@inproceedings{sarmasiHOADHanabiOpen2021,
  title      = {{{HOAD}}: {{The Hanabi Open Agent Dataset}}},
  shorttitle = {{{HOAD}}},
  booktitle  = {Proceedings of the 20th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  author     = {Sarmasi, Aron and Zhang, Timothy and Cheng, Chu-Hung and Pham, Huyen and Zhou, Xuanchen and Nguyen, Duong and Shekdar, Soumil and McCoy, Joshua},
  date       = {2021-05-03},
  series     = {{{AAMAS}} '21},
  pages      = {1646--1648},
  publisher  = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  location   = {Richland, SC},
  abstract   = {In this work we present the Hanabi Open Agent Dataset (HOAD)- meant to address the current lack of Hanabi datasets, HOAD is an easily extensible, open-sourced, and comprehensive collection of existing Hanabi playing agents, all ported to the Hanabi Learning Environment (HLE). We give a description and analysis of each agent's strategy, and we also show cross-play performance between all the agents, demonstrating both their high quality and diversity of strategy. These properties make HOAD especially well suited to studies involving meta-learning and transfer learning. Finally, we describe in detail an easy way to add new agents to HOAD regardless of the origin codebase of the agent and make our code and dataset publicly available at https://github.com/aronsar/hoad.},
  isbn       = {978-1-4503-8307-3},
  keywords   = {dataset,Hanabi}
}

@online{schaulPrioritizedExperienceReplay2016,
  title      = {Prioritized {{Experience Replay}}},
  author     = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date       = {2016-02-25},
  eprint     = {1511.05952},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.1511.05952},
  url        = {http://arxiv.org/abs/1511.05952},
  urldate    = {2024-10-28},
  abstract   = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/BPDPFP7U/Schaul et al. - 2016 - Prioritized Experience Replay.pdf;/home/skywalker/Zotero/storage/BPXZIX33/1511.html}
}

@online{schulmanProximalPolicyOptimization2017,
  title      = {Proximal {{Policy Optimization Algorithms}}},
  author     = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date       = {2017-08-28},
  eprint     = {1707.06347},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.1707.06347},
  url        = {http://arxiv.org/abs/1707.06347},
  urldate    = {2024-10-25},
  abstract   = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Machine Learning},
  file       = {/home/skywalker/Zotero/storage/H2YSZ8D5/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/skywalker/Zotero/storage/DE43I8KL/1707.html}
}

@article{shiptonCooperativeMultiAgentReinforcement2024,
  title        = {Cooperative {{Multi-Agent Reinforcement Learning}} in {{Convention Reliant Environments}}},
  author       = {Shipton, Jarrod},
  date         = {2024},
  journaltitle = {New Zealand},
  abstract     = {Multi-Agent Reinforcement Learning (MARL) has seen a move towards creating algorithms which can be trained to work cooperatively with partners. Typically MARL is done in self-play (SP). Recent works show agents trained with SP often achieve near optimal results when paired with one another, however, they form arbitrary play conventions which can perform poorly when mismatched. This led to research into algorithms which have been developed to form strategies which avoid the need for convention matching and allow for zero-shot coordination (ZSC) with any novel partner. ZSC solves the problem of convention matching, and is useful in short interactions, however in pro-longed or repeated interaction this comes at the cost of optimality. Avoiding conventions leaves the challenge of being unable to exploit known, existing conventions and achieve higher levels of optimality. In this work we use population training with a belief of the partner type to exploit conventions which could exist, leading to high rewards over pro-longed interactions. We demonstrate that our method is able to better adapt in convention reliant environments over repeated interactions than current state-of-the-art competing ZSC methods.},
  langid       = {english},
  file         = {/home/skywalker/Zotero/storage/N43R5M6Y/Shipton - 2024 - Cooperative Multi-Agent Reinforcement Learning in .pdf}
}

@inproceedings{sidjiHiddenRulesHanabi2023,
  title      = {The {{Hidden Rules}} of {{Hanabi}}: {{How Humans Outperform AI Agents}}},
  shorttitle = {The {{Hidden Rules}} of {{Hanabi}}},
  booktitle  = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Sidji, Matthew and Smith, Wally and Rogerson, Melissa J.},
  date       = {2023-04-19},
  series     = {{{CHI}} '23},
  pages      = {1--16},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3544548.3581550},
  url        = {https://doi.org/10.1145/3544548.3581550},
  urldate    = {2024-10-29},
  abstract   = {Games that feature multiple players, limited communication, and partial information are particularly challenging for AI agents. In the cooperative card game Hanabi, which possesses all of these attributes, AI agents fail to achieve scores comparable to even first-time human players. Through an observational study of three mixed-skill Hanabi play groups, we identify the techniques used by humans that help to explain their superior performance compared to AI. These concern physical artefact manipulation, coordination play, role establishment, and continual rule negotiation. Our findings extend previous accounts of human performance in Hanabi, which are purely in terms of theory-of-mind reasoning, by revealing more precisely how this form of collective decision-making is enacted in skilled human play. Our interpretation points to a gap in the current capabilities of AI agents to perform cooperative tasks.},
  isbn       = {978-1-4503-9421-5}
}

@article{silverMasteringGameGo2016,
  title        = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author       = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date         = {2016-01},
  journaltitle = {Nature},
  volume       = {529},
  number       = {7587},
  pages        = {484--489},
  publisher    = {Nature Publishing Group},
  issn         = {1476-4687},
  doi          = {10.1038/nature16961},
  url          = {https://www.nature.com/articles/nature16961},
  urldate      = {2024-10-25},
  abstract     = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  langid       = {english},
  keywords     = {Computational science,Computer science,Reward}
}

@inproceedings{siuEvaluationHumanAITeams2021,
  title     = {Evaluation of {{Human-AI Teams}} for {{Learned}} and {{Rule-Based Agents}} in {{Hanabi}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Siu, Ho Chit and Peña, Jaime and Chen, Edenna and Zhou, Yutai and Lopez, Victor and Palko, Kyle and Chang, Kimberlee and Allen, Ross},
  date      = {2021},
  volume    = {34},
  pages     = {16183--16195},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
  urldate   = {2024-04-16},
  abstract  = {Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance.},
  file      = {/home/skywalker/Zotero/storage/HQBLGM2F/Siu et al. - 2021 - Evaluation of Human-AI Teams for Learned and Rule-.pdf}
}

@incollection{spaanPartiallyObservableMarkov2012,
  title     = {Partially {{Observable Markov Decision Processes}}},
  booktitle = {Reinforcement {{Learning}}: {{State-of-the-Art}}},
  author    = {Spaan, Matthijs T. J.},
  editor    = {Wiering, Marco and Otterlo, Martijn},
  date      = {2012},
  pages     = {387--414},
  publisher = {Springer},
  location  = {Berlin, Heidelberg},
  doi       = {10.1007/978-3-642-27645-3_12},
  url       = {https://doi.org/10.1007/978-3-642-27645-3_12},
  urldate   = {2024-06-24},
  abstract  = {For reinforcement learning in environments in which an agent has access to a reliable state signal, methods based on the Markov decision process (MDP) have had many successes. In many problem domains, however, an agent suffers from limited sensing capabilities that preclude it from recovering a Markovian state signal from its perceptions. Extending the MDP framework, partially observable Markov decision processes (POMDPs) allow for principled decision making under conditions of uncertain sensing. In this chapter we present the POMDP model by focusing on the differences with fully observable MDPs, and we show how optimal policies for POMDPs can be represented. Next, we give a review of model-based techniques for policy computation, followed by an overview of the available model-free methods for POMDPs. We conclude by highlighting recent trends in POMDP reinforcement learning.},
  isbn      = {978-3-642-27645-3},
  langid    = {english}
}

@online{strouseCollaboratingHumansHuman2022,
  title       = {Collaborating with {{Humans}} without {{Human Data}}},
  author      = {Strouse, D. J. and McKee, Kevin R. and Botvinick, Matt and Hughes, Edward and Everett, Richard},
  date        = {2022-01-07},
  eprint      = {2110.08176},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2110.08176},
  urldate     = {2024-06-24},
  abstract    = {Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train "human-aware" agents ("behavioral cloning play", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file        = {/home/skywalker/Zotero/storage/79WLZUDI/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf;/home/skywalker/Zotero/storage/XSCENSLQ/2110.html}
}

@online{treutleinNewFormalismMethod2023,
  title       = {A {{New Formalism}}, {{Method}} and {{Open Issues}} for {{Zero-Shot Coordination}}},
  author      = {Treutlein, Johannes and Dennis, Michael and Oesterheld, Caspar and Foerster, Jakob},
  date        = {2023-07-12},
  eprint      = {2106.06613},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2106.06613},
  urldate     = {2024-05-09},
  abstract    = {In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multiagent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this “label-free” problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the labelfree coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, otherplay with tie-breaking, and prove that it is optimal in the LFC problem and an equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the ZSC setting aims to prevent, we conclude that the LFC problem does not reflect the aims of ZSC. To address this, we introduce an alternative informal operationalization of ZSC as a starting point for future work.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/KHUZGRUH/Treutlein et al. - 2023 - A New Formalism, Method and Open Issues for Zero-S.pdf}
}

@incollection{vanotterloReinforcementLearningMarkov2012,
  title     = {Reinforcement {{Learning}} and {{Markov Decision Processes}}},
  booktitle = {Reinforcement {{Learning}}},
  author    = {Van Otterlo, Martijn and Wiering, Marco},
  editor    = {Wiering, Marco and Van Otterlo, Martijn},
  date      = {2012},
  volume    = {12},
  pages     = {3--42},
  publisher = {Springer Berlin Heidelberg},
  location  = {Berlin, Heidelberg},
  doi       = {10.1007/978-3-642-27645-3_1},
  url       = {http://link.springer.com/10.1007/978-3-642-27645-3_1},
  urldate   = {2024-06-24},
  isbn      = {978-3-642-27644-6 978-3-642-27645-3},
  file      = {/home/skywalker/Zotero/storage/HKXS5GD9/Van Otterlo and Wiering - 2012 - Reinforcement Learning and Markov Decision Process.pdf}
}

@online{vezhnevetsFeUdalNetworksHierarchical2017,
  title      = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  author     = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  date       = {2017-03-06},
  eprint     = {1703.01161},
  eprinttype = {arXiv},
  doi        = {10.48550/arXiv.1703.01161},
  url        = {http://arxiv.org/abs/1703.01161},
  urldate    = {2024-10-25},
  abstract   = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
  pubstate   = {prepublished},
  keywords   = {Computer Science - Artificial Intelligence},
  file       = {/home/skywalker/Zotero/storage/8SBEYECJ/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf;/home/skywalker/Zotero/storage/9D2FV5NA/1703.html}
}

@online{walton-riversEvaluatingModellingHanabiPlaying2017,
  title       = {Evaluating and {{Modelling Hanabi-Playing Agents}}},
  author      = {Walton-Rivers, Joseph and Williams, Piers R. and Bartle, Richard and Perez-Liebana, Diego and Lucas, Simon M.},
  date        = {2017-04-24},
  eprint      = {1704.07069},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.1704.07069},
  url         = {http://arxiv.org/abs/1704.07069},
  urldate     = {2024-05-16},
  abstract    = {Agent modelling involves considering how other agents will behave, in order to influence your own actions. In this paper, we explore the use of agent modelling in the hidden-information, collaborative card game Hanabi. We implement a number of rule-based agents, both from the literature and of our own devising, in addition to an Information Set Monte Carlo Tree Search (IS-MCTS) agent. We observe poor results from IS-MCTS, so construct a new, predictor version that uses a model of the agents with which it is paired. We observe a significant improvement in game-playing strength from this agent in comparison to IS-MCTS, resulting from its consideration of what the other agents in a game would do. In addition, we create a flawed rule-based agent to highlight the predictor's capabilities with such an agent.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence},
  file        = {/home/skywalker/Zotero/storage/G6HGBY7Y/Walton-Rivers et al. - 2017 - Evaluating and Modelling Hanabi-Playing Agents.pdf;/home/skywalker/Zotero/storage/L3D2LAS3/1704.html}
}

@inproceedings{wangDuelingNetworkArchitectures2016,
  title      = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  booktitle  = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author     = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  date       = {2016-06-11},
  pages      = {1995--2003},
  publisher  = {PMLR},
  issn       = {1938-7228},
  url        = {https://proceedings.mlr.press/v48/wangf16.html},
  urldate    = {2024-10-28},
  abstract   = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid     = {english},
  file       = {/home/skywalker/Zotero/storage/G8SKWKI4/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforcement Learning.pdf}
}

@inproceedings{yanEfficientEndtoEndTraining2023,
  title      = {An {{Efficient End-to-End Training Approach}} for {{Zero-Shot Human-AI Coordination}}},
  author     = {Yan, Xue and Guo, Jiaxian and Lou, Xingzhou and Wang, Jun and Zhang, Haifeng and Du, Yali},
  date       = {2023-11-02},
  url        = {https://openreview.net/forum?id=6ePsuwXUwf},
  urldate    = {2024-06-25},
  abstract   = {The goal of zero-shot human-AI coordination is to develop an agent that can collaborate with humans without relying on human data. Prevailing two-stage population-based methods require a diverse population of mutually distinct policies to simulate diverse human behaviors. The necessity of such populations severely limits their computational efficiency. To address this issue, we propose E3T, an **E**fficient **E**nd-to-**E**nd **T**raining approach for zero-shot human-AI coordination. E3T employs a mixture of ego policy and random policy to construct the partner policy, making it both coordination-skilled and diverse. In this way, the ego agent is end-to-end trained with this mixture policy without the need of a pre-trained population, thus significantly improving the training efficiency. In addition, a partner modeling module is proposed to predict the partner's action from historical information. With the predicted partner's action, the ego policy is able to adapt its policy and take actions accordingly when collaborating with humans of different behavior patterns. Empirical results on the Overcooked environment show that our method significantly improves the training efficiency while preserving comparable or superior performance than the population-based baselines. Demo videos are available at https://sites.google.com/view/e3t-overcooked.},
  eventtitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  langid     = {english}
}

@article{yangEfficientTransferLearning2021,
  title        = {An Efficient Transfer Learning Framework for Multiagent Reinforcement Learning},
  author       = {Yang, Tianpei and Wang, Weixun and Tang, Hongyao and Hao, Jianye and Meng, Zhaopeng and Mao, Hangyu and Li, Dong and Liu, Wulong and Chen, Yingfeng and Hu, Yujing},
  date         = {2021},
  journaltitle = {Advances in neural information processing systems},
  volume       = {34},
  pages        = {17037--17048},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/8d9a6e908ed2b731fb96151d9bb94d49-Abstract.html},
  urldate      = {2024-06-03},
  file         = {/home/skywalker/Zotero/storage/RYWMQDPT/Yang et al. - 2021 - An efficient transfer learning framework for multi.pdf}
}

@online{yuanSurveyProgressCooperative2023,
  title       = {A {{Survey}} of {{Progress}} on {{Cooperative Multi-agent Reinforcement Learning}} in {{Open Environment}}},
  author      = {Yuan, Lei and Zhang, Ziqian and Li, Lihe and Guan, Cong and Yu, Yang},
  date        = {2023-12-02},
  eprint      = {2312.01058},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2312.01058},
  url         = {http://arxiv.org/abs/2312.01058},
  urldate     = {2024-05-09},
  abstract    = {Multi-agent Reinforcement Learning (MARL) has gained wide attention in recent years and has made progress in various fields. Specifically, cooperative MARL focuses on training a team of agents to cooperatively achieve tasks that are difficult for a single agent to handle. It has shown great potential in applications such as path planning, autonomous driving, active voltage control, and dynamic algorithm configuration. One of the research focuses in the field of cooperative MARL is how to improve the coordination efficiency of the system, while research work has mainly been conducted in simple, static, and closed environment settings. To promote the application of artificial intelligence in real-world, some research has begun to explore multi-agent coordination in open environments. These works have made progress in exploring and researching the environments where important factors might change. However, the mainstream work still lacks a comprehensive review of the research direction. In this paper, starting from the concept of reinforcement learning, we subsequently introduce multi-agent systems (MAS), cooperative MARL, typical methods, and test environments. Then, we summarize the research work of cooperative MARL from closed to open environments, extract multiple research directions, and introduce typical works. Finally, we summarize the strengths and weaknesses of the current research, and look forward to the future development direction and research problems in cooperative MARL in open environments.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Multiagent Systems},
  file        = {/home/skywalker/Zotero/storage/6MFCK54D/Yuan et al. - 2023 - A Survey of Progress on Cooperative Multi-agent Re.pdf;/home/skywalker/Zotero/storage/8MIXYVR7/2312.html}
}

@online{yuSurprisingEffectivenessPPO2022,
  title       = {The {{Surprising Effectiveness}} of {{PPO}} in {{Cooperative}}, {{Multi-Agent Games}}},
  author      = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  date        = {2022-11-04},
  eprint      = {2103.01955},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2103.01955},
  url         = {http://arxiv.org/abs/2103.01955},
  urldate     = {2024-04-17},
  abstract    = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at \textbackslash url\{https://github.com/marlbenchmark/on-policy\}.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file        = {/home/skywalker/Zotero/storage/36XPBXWW/Yu et al. - 2022 - The Surprising Effectiveness of PPO in Cooperative.pdf;/home/skywalker/Zotero/storage/TZV3NS36/2103.html}
}

@online{zandOntheflyStrategyAdaptation2022,
  title       = {On-the-Fly {{Strategy Adaptation}} for Ad-Hoc {{Agent Coordination}}},
  author      = {Zand, Jaleh and Parker-Holder, Jack and Roberts, Stephen J.},
  date        = {2022-03-07},
  eprint      = {2203.08015},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  doi         = {10.48550/arXiv.2203.08015},
  url         = {http://arxiv.org/abs/2203.08015},
  urldate     = {2024-04-16},
  abstract    = {Training agents in cooperative settings offers the promise of AI agents able to interact effectively with humans (and other agents) in the real world. Multi-agent reinforcement learning (MARL) has the potential to achieve this goal, demonstrating success in a series of challenging problems. However, whilst these advances are significant, the vast majority of focus has been on the self-play paradigm. This often results in a coordination problem, caused by agents learning to make use of arbitrary conventions when playing with themselves. This means that even the strongest self-play agents may have very low cross-play with other agents, including other initializations of the same algorithm. In this paper we propose to solve this problem by adapting agent strategies on the fly, using a posterior belief over the other agents' strategy. Concretely, we consider the problem of selecting a strategy from a finite set of previously trained agents, to play with an unknown partner. We propose an extension of the classic statistical technique, Gibbs sampling, to update beliefs about other agents and obtain close to optimal ad-hoc performance. Despite its simplicity, our method is able to achieve strong cross-play with unseen partners in the challenging card game of Hanabi, achieving successful ad-hoc coordination without knowledge of the partner's strategy a priori.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file        = {/home/skywalker/Zotero/storage/VGTTIAK9/Zand et al. - 2022 - On-the-fly Strategy Adaptation for ad-hoc Agent Co.pdf;/home/skywalker/Zotero/storage/U4LRCLU3/2203.html}
}

@inproceedings{zhaoImprovedDeepReinforcement2023,
  title     = {An {{Improved Deep Reinforcement Learning-Based Multi-Agent Cooperative Game Approach}}},
  booktitle = {2023 {{International Conference}} on {{High Performance Big Data}} and {{Intelligent Systems}} ({{HDIS}})},
  author    = {Zhao, Zhongqi and Zhang, Chuang and Xu, Haoran and Kou, Jiawei and Cheng, Hui},
  date      = {2023},
  pages     = {200--203},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/abstract/document/10499468/},
  urldate   = {2024-06-24}
}
