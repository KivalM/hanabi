{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
      "  self.hub = sentry_sdk.Hub(client)\n"
     ]
    }
   ],
   "source": [
    "from hanabi.src.agent.dqn.train import DQNTrainer, TrainConfig\n",
    "\n",
    "config = TrainConfig(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkivalm\u001b[0m (\u001b[33mkivalm-University of KwaZulu-Natal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/HonorsProject/code/wandb/run-20240829_012244-z0mn0mpj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/z0mn0mpj' target=\"_blank\">resilient-plant-77</a></strong> to <a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors' target=\"_blank\">https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/z0mn0mpj' target=\"_blank\">https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/z0mn0mpj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/z0mn0mpj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbf09952af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Honors\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1002: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Epsilon: 0.49950999999998036, Loss: 1.2435725927352905\n",
      "Step: 2000, Epsilon: 0.4990199999999607, Loss: 0.8359941244125366\n",
      "Step: 3000, Epsilon: 0.4985299999999411, Loss: 0.8957824110984802\n",
      "Step: 4000, Epsilon: 0.49803999999992143, Loss: 0.7913891673088074\n",
      "Step: 5000, Epsilon: 0.4975499999999018, Loss: 0.7733955979347229\n",
      "Step: 6000, Epsilon: 0.49705999999988215, Loss: 0.7502119541168213\n",
      "Step: 7000, Epsilon: 0.4965699999998625, Loss: 0.8007124066352844\n",
      "Step: 8000, Epsilon: 0.49607999999984287, Loss: 0.7372831702232361\n",
      "Step: 9000, Epsilon: 0.4955899999998232, Loss: 0.6711556315422058\n",
      "Step: 10000, Epsilon: 0.4950999999998036, Loss: 0.6857780814170837\n",
      "Step: 11000, Epsilon: 0.49460999999978394, Loss: 0.6145299077033997\n",
      "Step: 12000, Epsilon: 0.4941199999997643, Loss: 0.47295722365379333\n",
      "Step: 13000, Epsilon: 0.49362999999974466, Loss: 0.6171962022781372\n",
      "Step: 14000, Epsilon: 0.493139999999725, Loss: 0.5638169646263123\n",
      "Step: 15000, Epsilon: 0.4926499999997054, Loss: 0.528086245059967\n",
      "Step: 16000, Epsilon: 0.49215999999968574, Loss: 0.5308570861816406\n",
      "Step: 17000, Epsilon: 0.4916699999996661, Loss: 0.5173627138137817\n",
      "Step: 18000, Epsilon: 0.49117999999964645, Loss: 0.5912917256355286\n",
      "Step: 19000, Epsilon: 0.4906899999996268, Loss: 0.4134425222873688\n",
      "Step: 20000, Epsilon: 0.49019999999960717, Loss: 0.4675944745540619\n",
      "Step: 21000, Epsilon: 0.48970999999958753, Loss: 0.4553482234477997\n",
      "Step: 22000, Epsilon: 0.4892199999995679, Loss: 0.5270729660987854\n",
      "Step: 23000, Epsilon: 0.48872999999954825, Loss: 0.6859334707260132\n",
      "Step: 24000, Epsilon: 0.4882399999995286, Loss: 0.4479532539844513\n",
      "Step: 25000, Epsilon: 0.48774999999950897, Loss: 0.4891080856323242\n",
      "Step: 26000, Epsilon: 0.4872599999994893, Loss: 0.5506784915924072\n",
      "Step: 27000, Epsilon: 0.4867699999994697, Loss: 0.4486030042171478\n",
      "Step: 28000, Epsilon: 0.48627999999945004, Loss: 0.5039175152778625\n",
      "Step: 29000, Epsilon: 0.4857899999994304, Loss: 0.5282169580459595\n",
      "Step: 30000, Epsilon: 0.48529999999941076, Loss: 0.5093668103218079\n",
      "Step: 31000, Epsilon: 0.4848099999993911, Loss: 0.4291539490222931\n",
      "Step: 32000, Epsilon: 0.4843199999993715, Loss: 0.5881281495094299\n",
      "Step: 33000, Epsilon: 0.48382999999935183, Loss: 0.4839145243167877\n",
      "Step: 34000, Epsilon: 0.4833399999993322, Loss: 0.5141080021858215\n",
      "Step: 35000, Epsilon: 0.48284999999931255, Loss: 0.535810112953186\n",
      "Step: 36000, Epsilon: 0.4823599999992929, Loss: 0.492438405752182\n",
      "Step: 37000, Epsilon: 0.48186999999927327, Loss: 0.49674099683761597\n",
      "Step: 38000, Epsilon: 0.4813799999992536, Loss: 0.5750122666358948\n",
      "Step: 39000, Epsilon: 0.480889999999234, Loss: 0.5354982614517212\n",
      "Step: 40000, Epsilon: 0.48039999999921434, Loss: 0.5103312730789185\n",
      "Step: 41000, Epsilon: 0.4799099999991947, Loss: 0.5547624826431274\n",
      "Step: 42000, Epsilon: 0.47941999999917506, Loss: 0.5856208801269531\n",
      "Step: 43000, Epsilon: 0.4789299999991554, Loss: 0.4984261691570282\n",
      "Step: 44000, Epsilon: 0.4784399999991358, Loss: 0.49525320529937744\n",
      "Step: 45000, Epsilon: 0.47794999999911614, Loss: 0.5262154340744019\n",
      "Step: 46000, Epsilon: 0.4774599999990965, Loss: 0.5383616089820862\n",
      "Step: 47000, Epsilon: 0.47696999999907685, Loss: 0.620121955871582\n",
      "Step: 48000, Epsilon: 0.4764799999990572, Loss: 0.62295001745224\n",
      "Step: 49000, Epsilon: 0.47598999999903757, Loss: 0.6379272937774658\n",
      "Step: 50000, Epsilon: 0.47549999999901793, Loss: 0.5466180443763733\n",
      "Step: 51000, Epsilon: 0.4750099999989983, Loss: 0.5734898447990417\n",
      "Step: 52000, Epsilon: 0.47451999999897865, Loss: 0.639549732208252\n",
      "Step: 53000, Epsilon: 0.474029999998959, Loss: 0.5706462860107422\n",
      "Step: 54000, Epsilon: 0.47353999999893936, Loss: 0.5211428999900818\n",
      "Step: 55000, Epsilon: 0.4730499999989197, Loss: 0.5447225570678711\n",
      "Step: 56000, Epsilon: 0.4725599999989001, Loss: 0.6237863302230835\n",
      "Step: 57000, Epsilon: 0.47206999999888044, Loss: 0.6297569870948792\n",
      "Step: 58000, Epsilon: 0.4715799999988608, Loss: 0.39107656478881836\n",
      "Step: 59000, Epsilon: 0.47108999999884116, Loss: 0.5374153852462769\n",
      "Step: 60000, Epsilon: 0.4705999999988215, Loss: 0.47528788447380066\n",
      "Step: 61000, Epsilon: 0.4701099999988019, Loss: 0.6267562508583069\n",
      "Step: 62000, Epsilon: 0.46961999999878223, Loss: 0.5398088097572327\n",
      "Step: 63000, Epsilon: 0.4691299999987626, Loss: 0.57220059633255\n",
      "Step: 64000, Epsilon: 0.46863999999874295, Loss: 0.5776798725128174\n",
      "Step: 65000, Epsilon: 0.4681499999987233, Loss: 0.6604228615760803\n",
      "Step: 66000, Epsilon: 0.46765999999870367, Loss: 0.45662352442741394\n",
      "Step: 67000, Epsilon: 0.467169999998684, Loss: 0.5058798789978027\n",
      "Step: 68000, Epsilon: 0.4666799999986644, Loss: 0.4604596495628357\n",
      "Step: 69000, Epsilon: 0.46618999999864474, Loss: 0.4619261622428894\n",
      "Step: 70000, Epsilon: 0.4656999999986251, Loss: 0.5263561606407166\n",
      "Step: 71000, Epsilon: 0.46520999999860546, Loss: 0.5910372138023376\n",
      "Step: 72000, Epsilon: 0.4647199999985858, Loss: 0.6100431680679321\n",
      "Step: 73000, Epsilon: 0.4642299999985662, Loss: 0.5591935515403748\n",
      "Step: 74000, Epsilon: 0.46373999999854654, Loss: 0.4633456766605377\n",
      "Step: 75000, Epsilon: 0.4632499999985269, Loss: 0.47948768734931946\n",
      "Step: 76000, Epsilon: 0.46275999999850725, Loss: 0.49124178290367126\n",
      "Step: 77000, Epsilon: 0.4622699999984876, Loss: 0.5503152012825012\n",
      "Step: 78000, Epsilon: 0.46177999999846797, Loss: 0.6173480749130249\n",
      "Step: 79000, Epsilon: 0.46128999999844833, Loss: 0.521087110042572\n",
      "Step: 80000, Epsilon: 0.4607999999984287, Loss: 0.5127971172332764\n",
      "Step: 81000, Epsilon: 0.46030999999840905, Loss: 0.5423914194107056\n",
      "Step: 82000, Epsilon: 0.4598199999983894, Loss: 0.4753393530845642\n",
      "Step: 83000, Epsilon: 0.45932999999836976, Loss: 0.46434009075164795\n",
      "Step: 84000, Epsilon: 0.4588399999983501, Loss: 0.4853701889514923\n",
      "Step: 85000, Epsilon: 0.4583499999983305, Loss: 0.5476675629615784\n",
      "Step: 86000, Epsilon: 0.45785999999831084, Loss: 0.5462698340415955\n",
      "Step: 87000, Epsilon: 0.4573699999982912, Loss: 0.46519753336906433\n",
      "Step: 88000, Epsilon: 0.45687999999827156, Loss: 0.49652475118637085\n",
      "Step: 89000, Epsilon: 0.4563899999982519, Loss: 0.5078765749931335\n",
      "Step: 90000, Epsilon: 0.4558999999982323, Loss: 0.5036890506744385\n",
      "Step: 91000, Epsilon: 0.45540999999821263, Loss: 0.4691053628921509\n",
      "Step: 92000, Epsilon: 0.454919999998193, Loss: 0.46001389622688293\n",
      "Step: 93000, Epsilon: 0.45442999999817335, Loss: 0.5027705430984497\n",
      "Step: 94000, Epsilon: 0.4539399999981537, Loss: 0.5278792977333069\n",
      "Step: 95000, Epsilon: 0.45344999999813407, Loss: 0.5492531061172485\n",
      "Step: 96000, Epsilon: 0.4529599999981144, Loss: 0.4355182647705078\n",
      "Step: 97000, Epsilon: 0.4524699999980948, Loss: 0.5778904557228088\n",
      "Step: 98000, Epsilon: 0.45197999999807514, Loss: 0.5398885607719421\n",
      "Step: 99000, Epsilon: 0.4514899999980555, Loss: 0.4818919897079468\n",
      "Step: 100000, Epsilon: 0.45099999999803586, Loss: 0.5152110457420349\n",
      "Step: 101000, Epsilon: 0.4505099999980162, Loss: 0.5333479046821594\n",
      "Step: 102000, Epsilon: 0.4500199999979966, Loss: 0.43741077184677124\n",
      "Step: 103000, Epsilon: 0.44952999999797694, Loss: 0.3965270519256592\n",
      "Step: 104000, Epsilon: 0.4490399999979573, Loss: 0.5245551466941833\n",
      "Step: 105000, Epsilon: 0.44854999999793765, Loss: 0.5236761569976807\n",
      "Step: 106000, Epsilon: 0.448059999997918, Loss: 0.5423605442047119\n",
      "Step: 107000, Epsilon: 0.44756999999789837, Loss: 0.47836658358573914\n",
      "Step: 108000, Epsilon: 0.44707999999787873, Loss: 0.54337078332901\n",
      "Step: 109000, Epsilon: 0.4465899999978591, Loss: 0.5319269299507141\n",
      "Step: 110000, Epsilon: 0.44609999999783945, Loss: 0.5446871519088745\n",
      "Step: 111000, Epsilon: 0.4456099999978198, Loss: 0.41258612275123596\n",
      "Step: 112000, Epsilon: 0.44511999999780016, Loss: 0.44556599855422974\n",
      "Step: 113000, Epsilon: 0.4446299999977805, Loss: 0.5325204133987427\n",
      "Step: 114000, Epsilon: 0.4441399999977609, Loss: 0.44574564695358276\n",
      "Step: 115000, Epsilon: 0.44364999999774124, Loss: 0.583625078201294\n",
      "Step: 116000, Epsilon: 0.4431599999977216, Loss: 0.5217269659042358\n",
      "Step: 117000, Epsilon: 0.44266999999770196, Loss: 0.4335132837295532\n",
      "Step: 118000, Epsilon: 0.4421799999976823, Loss: 0.5169362425804138\n",
      "Step: 119000, Epsilon: 0.4416899999976627, Loss: 0.5918236970901489\n",
      "Step: 120000, Epsilon: 0.44119999999764303, Loss: 0.5172136425971985\n",
      "Step: 121000, Epsilon: 0.4407099999976234, Loss: 0.4289633631706238\n",
      "Step: 122000, Epsilon: 0.44021999999760375, Loss: 0.5739157795906067\n",
      "Step: 123000, Epsilon: 0.4397299999975841, Loss: 0.46638578176498413\n",
      "Step: 124000, Epsilon: 0.43923999999756447, Loss: 0.40753090381622314\n",
      "Step: 125000, Epsilon: 0.4387499999975448, Loss: 0.46041932702064514\n",
      "Step: 126000, Epsilon: 0.4382599999975252, Loss: 0.4783763587474823\n",
      "Step: 127000, Epsilon: 0.43776999999750554, Loss: 0.584700345993042\n",
      "Step: 128000, Epsilon: 0.4372799999974859, Loss: 0.4752204418182373\n",
      "Step: 129000, Epsilon: 0.43678999999746626, Loss: 0.45170095562934875\n",
      "Step: 130000, Epsilon: 0.4362999999974466, Loss: 0.4504828155040741\n",
      "Step: 131000, Epsilon: 0.435809999997427, Loss: 0.5291230082511902\n",
      "Step: 132000, Epsilon: 0.43531999999740734, Loss: 0.5311692953109741\n",
      "Step: 133000, Epsilon: 0.4348299999973877, Loss: 0.5111464858055115\n",
      "Step: 134000, Epsilon: 0.43433999999736805, Loss: 0.37571415305137634\n",
      "Step: 135000, Epsilon: 0.4338499999973484, Loss: 0.5387099385261536\n",
      "Step: 136000, Epsilon: 0.43335999999732877, Loss: 0.44426247477531433\n",
      "Step: 137000, Epsilon: 0.43286999999730913, Loss: 0.3879368007183075\n",
      "Step: 138000, Epsilon: 0.4323799999972895, Loss: 0.40523216128349304\n",
      "Step: 139000, Epsilon: 0.43188999999726985, Loss: 0.5034605264663696\n",
      "Step: 140000, Epsilon: 0.4313999999972502, Loss: 0.45993220806121826\n",
      "Step: 141000, Epsilon: 0.43090999999723056, Loss: 0.43100395798683167\n",
      "Step: 142000, Epsilon: 0.4304199999972109, Loss: 0.42874675989151\n",
      "Step: 143000, Epsilon: 0.4299299999971913, Loss: 0.4199768304824829\n",
      "Step: 144000, Epsilon: 0.42943999999717164, Loss: 0.42054852843284607\n",
      "Step: 145000, Epsilon: 0.428949999997152, Loss: 0.4875171184539795\n",
      "Step: 146000, Epsilon: 0.42845999999713236, Loss: 0.4369559586048126\n",
      "Step: 147000, Epsilon: 0.4279699999971127, Loss: 0.5063031911849976\n",
      "Step: 148000, Epsilon: 0.4274799999970931, Loss: 0.30275219678878784\n",
      "Step: 149000, Epsilon: 0.42698999999707343, Loss: 0.4403620958328247\n",
      "Step: 150000, Epsilon: 0.4264999999970538, Loss: 0.355925977230072\n",
      "Step: 151000, Epsilon: 0.42600999999703415, Loss: 0.48522835969924927\n",
      "Step: 152000, Epsilon: 0.4255199999970145, Loss: 0.3677338659763336\n",
      "Step: 153000, Epsilon: 0.42502999999699487, Loss: 0.36176803708076477\n",
      "Step: 154000, Epsilon: 0.4245399999969752, Loss: 0.3712945878505707\n",
      "Step: 155000, Epsilon: 0.4240499999969556, Loss: 0.4155193865299225\n",
      "Step: 156000, Epsilon: 0.42355999999693594, Loss: 0.3946872353553772\n",
      "Step: 157000, Epsilon: 0.4230699999969163, Loss: 0.3352065980434418\n",
      "Step: 158000, Epsilon: 0.42257999999689666, Loss: 0.3605518937110901\n",
      "Step: 159000, Epsilon: 0.422089999996877, Loss: 0.4000416100025177\n",
      "Step: 160000, Epsilon: 0.4215999999968574, Loss: 0.44855019450187683\n",
      "Step: 161000, Epsilon: 0.42110999999683774, Loss: 0.32840171456336975\n",
      "Step: 162000, Epsilon: 0.4206199999968181, Loss: 0.39908725023269653\n",
      "Step: 163000, Epsilon: 0.42012999999679845, Loss: 0.4642527103424072\n",
      "Step: 164000, Epsilon: 0.4196399999967788, Loss: 0.48567044734954834\n",
      "Step: 165000, Epsilon: 0.41914999999675917, Loss: 0.4155721068382263\n",
      "Step: 166000, Epsilon: 0.41865999999673953, Loss: 0.40471839904785156\n",
      "Step: 167000, Epsilon: 0.4181699999967199, Loss: 0.542863130569458\n",
      "Step: 168000, Epsilon: 0.41767999999670025, Loss: 0.41038525104522705\n",
      "Step: 169000, Epsilon: 0.4171899999966806, Loss: 0.5653641223907471\n",
      "Step: 170000, Epsilon: 0.41669999999666096, Loss: 0.5076925754547119\n",
      "Step: 171000, Epsilon: 0.4162099999966413, Loss: 0.5623440742492676\n",
      "Step: 172000, Epsilon: 0.4157199999966217, Loss: 0.4332941770553589\n",
      "Step: 173000, Epsilon: 0.41522999999660204, Loss: 0.44395169615745544\n",
      "Step: 174000, Epsilon: 0.4147399999965824, Loss: 0.4744265377521515\n",
      "Step: 175000, Epsilon: 0.41424999999656276, Loss: 0.4333840310573578\n",
      "Step: 176000, Epsilon: 0.4137599999965431, Loss: 0.5270864367485046\n",
      "Step: 177000, Epsilon: 0.4132699999965235, Loss: 0.6013450026512146\n",
      "Step: 178000, Epsilon: 0.41277999999650383, Loss: 0.45831912755966187\n",
      "Step: 179000, Epsilon: 0.4122899999964842, Loss: 0.3470095098018646\n",
      "Step: 180000, Epsilon: 0.41179999999646455, Loss: 0.5196205377578735\n",
      "Step: 181000, Epsilon: 0.4113099999964449, Loss: 0.3945869505405426\n",
      "Step: 182000, Epsilon: 0.41081999999642527, Loss: 0.45520976185798645\n",
      "Step: 183000, Epsilon: 0.4103299999964056, Loss: 0.4116878807544708\n",
      "Step: 184000, Epsilon: 0.409839999996386, Loss: 0.41810500621795654\n",
      "Step: 185000, Epsilon: 0.40934999999636634, Loss: 0.49165281653404236\n",
      "Step: 186000, Epsilon: 0.4088599999963467, Loss: 0.3833918273448944\n",
      "Step: 187000, Epsilon: 0.40836999999632706, Loss: 0.3661527931690216\n",
      "Step: 188000, Epsilon: 0.4078799999963074, Loss: 0.39235126972198486\n",
      "Step: 189000, Epsilon: 0.4073899999962878, Loss: 0.36523255705833435\n",
      "Step: 190000, Epsilon: 0.40689999999626814, Loss: 0.39860811829566956\n",
      "Step: 191000, Epsilon: 0.4064099999962485, Loss: 0.42072343826293945\n",
      "Step: 192000, Epsilon: 0.40591999999622885, Loss: 0.5058727264404297\n",
      "Step: 193000, Epsilon: 0.4054299999962092, Loss: 0.45638659596443176\n",
      "Step: 194000, Epsilon: 0.40493999999618957, Loss: 0.3511400818824768\n",
      "Step: 195000, Epsilon: 0.40444999999616993, Loss: 0.38909927010536194\n",
      "Step: 196000, Epsilon: 0.4039599999961503, Loss: 0.44684696197509766\n",
      "Step: 197000, Epsilon: 0.40346999999613065, Loss: 0.5298020243644714\n",
      "Step: 198000, Epsilon: 0.402979999996111, Loss: 0.48268210887908936\n",
      "Step: 199000, Epsilon: 0.40248999999609136, Loss: 0.4134851396083832\n",
      "Step: 200000, Epsilon: 0.4019999999960717, Loss: 0.4457434415817261\n",
      "Step: 201000, Epsilon: 0.4015099999960521, Loss: 0.48288241028785706\n",
      "Step: 202000, Epsilon: 0.40101999999603244, Loss: 0.4197097420692444\n",
      "Step: 203000, Epsilon: 0.4005299999960128, Loss: 0.41962069272994995\n",
      "Step: 204000, Epsilon: 0.40003999999599316, Loss: 0.4888288378715515\n",
      "Step: 205000, Epsilon: 0.3995499999959735, Loss: 0.4189063310623169\n",
      "Step: 206000, Epsilon: 0.3990599999959539, Loss: 0.549755871295929\n",
      "Step: 207000, Epsilon: 0.39856999999593423, Loss: 0.493962824344635\n",
      "Step: 208000, Epsilon: 0.3980799999959146, Loss: 0.4540799558162689\n",
      "Step: 209000, Epsilon: 0.39758999999589495, Loss: 0.4892425537109375\n",
      "Step: 210000, Epsilon: 0.3970999999958753, Loss: 0.5069097280502319\n",
      "Step: 211000, Epsilon: 0.39660999999585567, Loss: 0.4425550103187561\n",
      "Step: 212000, Epsilon: 0.396119999995836, Loss: 0.5824469327926636\n",
      "Step: 213000, Epsilon: 0.3956299999958164, Loss: 0.5586078763008118\n",
      "Step: 214000, Epsilon: 0.39513999999579674, Loss: 0.4441433548927307\n",
      "Step: 215000, Epsilon: 0.3946499999957771, Loss: 0.32294535636901855\n",
      "Step: 216000, Epsilon: 0.39415999999575746, Loss: 0.48014578223228455\n",
      "Step: 217000, Epsilon: 0.3936699999957378, Loss: 0.45485565066337585\n",
      "Step: 218000, Epsilon: 0.3931799999957182, Loss: 0.5190554857254028\n",
      "Step: 219000, Epsilon: 0.39268999999569854, Loss: 0.5029934048652649\n",
      "Step: 220000, Epsilon: 0.3921999999956789, Loss: 0.39028412103652954\n",
      "Step: 221000, Epsilon: 0.39170999999565925, Loss: 0.39908674359321594\n",
      "Step: 222000, Epsilon: 0.3912199999956396, Loss: 0.3293624222278595\n",
      "Step: 223000, Epsilon: 0.39072999999561997, Loss: 0.38596370816230774\n",
      "Step: 224000, Epsilon: 0.39023999999560033, Loss: 0.3814510703086853\n",
      "Step: 225000, Epsilon: 0.3897499999955807, Loss: 0.3600619435310364\n",
      "Step: 226000, Epsilon: 0.38925999999556105, Loss: 0.478031724691391\n",
      "Step: 227000, Epsilon: 0.3887699999955414, Loss: 0.3736966550350189\n",
      "Step: 228000, Epsilon: 0.38827999999552176, Loss: 0.3407670855522156\n",
      "Step: 229000, Epsilon: 0.3877899999955021, Loss: 0.5568661093711853\n",
      "Step: 230000, Epsilon: 0.3872999999954825, Loss: 0.3949294686317444\n",
      "Step: 231000, Epsilon: 0.38680999999546284, Loss: 0.4160023331642151\n",
      "Step: 232000, Epsilon: 0.3863199999954432, Loss: 0.39921310544013977\n",
      "Step: 233000, Epsilon: 0.38582999999542356, Loss: 0.34697991609573364\n",
      "Step: 234000, Epsilon: 0.3853399999954039, Loss: 0.38660141825675964\n",
      "Step: 235000, Epsilon: 0.3848499999953843, Loss: 0.44586434960365295\n",
      "Step: 236000, Epsilon: 0.38435999999536463, Loss: 0.380892276763916\n",
      "Step: 237000, Epsilon: 0.383869999995345, Loss: 0.32504647970199585\n",
      "Step: 238000, Epsilon: 0.38337999999532535, Loss: 0.4245585799217224\n",
      "Step: 239000, Epsilon: 0.3828899999953057, Loss: 0.47902005910873413\n",
      "Step: 240000, Epsilon: 0.38239999999528607, Loss: 0.34777748584747314\n",
      "Step: 241000, Epsilon: 0.3819099999952664, Loss: 0.3533206880092621\n",
      "Step: 242000, Epsilon: 0.3814199999952468, Loss: 0.3722662627696991\n",
      "Step: 243000, Epsilon: 0.38092999999522714, Loss: 0.3615565299987793\n",
      "Step: 244000, Epsilon: 0.3804399999952075, Loss: 0.35437244176864624\n",
      "Step: 245000, Epsilon: 0.37994999999518786, Loss: 0.3356124460697174\n",
      "Step: 246000, Epsilon: 0.3794599999951682, Loss: 0.35690799355506897\n",
      "Step: 247000, Epsilon: 0.3789699999951486, Loss: 0.4209127724170685\n",
      "Step: 248000, Epsilon: 0.37847999999512894, Loss: 0.29859820008277893\n",
      "Step: 249000, Epsilon: 0.3779899999951093, Loss: 0.40987733006477356\n",
      "Step: 250000, Epsilon: 0.37749999999508965, Loss: 0.3029833436012268\n",
      "Step: 251000, Epsilon: 0.37700999999507, Loss: 0.27179184556007385\n",
      "Step: 252000, Epsilon: 0.37651999999505037, Loss: 0.3874322772026062\n",
      "Step: 253000, Epsilon: 0.37602999999503073, Loss: 0.31386083364486694\n",
      "Step: 254000, Epsilon: 0.3755399999950111, Loss: 0.40404340624809265\n",
      "Step: 255000, Epsilon: 0.37504999999499145, Loss: 0.33308109641075134\n",
      "Step: 256000, Epsilon: 0.3745599999949718, Loss: 0.3919886648654938\n",
      "Step: 257000, Epsilon: 0.37406999999495216, Loss: 0.30761560797691345\n",
      "Step: 258000, Epsilon: 0.3735799999949325, Loss: 0.42904266715049744\n",
      "Step: 259000, Epsilon: 0.3730899999949129, Loss: 0.34932461380958557\n",
      "Step: 260000, Epsilon: 0.37259999999489324, Loss: 0.35900673270225525\n",
      "Step: 261000, Epsilon: 0.3721099999948736, Loss: 0.45544081926345825\n",
      "Step: 262000, Epsilon: 0.37161999999485396, Loss: 0.3388504087924957\n",
      "Step: 263000, Epsilon: 0.3711299999948343, Loss: 0.4145767390727997\n",
      "Step: 264000, Epsilon: 0.3706399999948147, Loss: 0.3532221019268036\n",
      "Step: 265000, Epsilon: 0.37014999999479503, Loss: 0.36489710211753845\n",
      "Step: 266000, Epsilon: 0.3696599999947754, Loss: 0.35476160049438477\n",
      "Step: 267000, Epsilon: 0.36916999999475575, Loss: 0.3280283510684967\n",
      "Step: 268000, Epsilon: 0.3686799999947361, Loss: 0.41280585527420044\n",
      "Step: 269000, Epsilon: 0.36818999999471647, Loss: 0.37576040625572205\n",
      "Step: 270000, Epsilon: 0.3676999999946968, Loss: 0.33327680826187134\n",
      "Step: 271000, Epsilon: 0.3672099999946772, Loss: 0.4108152687549591\n",
      "Step: 272000, Epsilon: 0.36671999999465754, Loss: 0.3698074221611023\n",
      "Step: 273000, Epsilon: 0.3662299999946379, Loss: 0.37235188484191895\n",
      "Step: 274000, Epsilon: 0.36573999999461826, Loss: 0.3313128352165222\n",
      "Step: 275000, Epsilon: 0.3652499999945986, Loss: 0.34439876675605774\n",
      "Step: 276000, Epsilon: 0.364759999994579, Loss: 0.3949239253997803\n",
      "Step: 277000, Epsilon: 0.36426999999455933, Loss: 0.39406612515449524\n",
      "Step: 278000, Epsilon: 0.3637799999945397, Loss: 0.2858749330043793\n",
      "Step: 279000, Epsilon: 0.36328999999452005, Loss: 0.3393644094467163\n",
      "Step: 280000, Epsilon: 0.3627999999945004, Loss: 0.3388850688934326\n",
      "Step: 281000, Epsilon: 0.36230999999448077, Loss: 0.423645943403244\n",
      "Step: 282000, Epsilon: 0.36181999999446113, Loss: 0.4011985957622528\n",
      "Step: 283000, Epsilon: 0.3613299999944415, Loss: 0.32483676075935364\n",
      "Step: 284000, Epsilon: 0.36083999999442185, Loss: 0.3459125757217407\n",
      "Step: 285000, Epsilon: 0.3603499999944022, Loss: 0.4575151801109314\n",
      "Step: 286000, Epsilon: 0.35985999999438256, Loss: 0.3595867455005646\n",
      "Step: 287000, Epsilon: 0.3593699999943629, Loss: 0.4301159083843231\n",
      "Step: 288000, Epsilon: 0.3588799999943433, Loss: 0.3800528347492218\n",
      "Step: 289000, Epsilon: 0.35838999999432364, Loss: 0.4076457917690277\n",
      "Step: 290000, Epsilon: 0.357899999994304, Loss: 0.4331827163696289\n",
      "Step: 291000, Epsilon: 0.35740999999428436, Loss: 0.4174706041812897\n",
      "Step: 292000, Epsilon: 0.3569199999942647, Loss: 0.5240141749382019\n",
      "Step: 293000, Epsilon: 0.3564299999942451, Loss: 0.40066996216773987\n",
      "Step: 294000, Epsilon: 0.35593999999422543, Loss: 0.5940783023834229\n",
      "Step: 295000, Epsilon: 0.3554499999942058, Loss: 0.5260136723518372\n",
      "Step: 296000, Epsilon: 0.35495999999418615, Loss: 0.531169593334198\n",
      "Step: 297000, Epsilon: 0.3544699999941665, Loss: 0.47633954882621765\n",
      "Step: 298000, Epsilon: 0.35397999999414687, Loss: 0.3919384777545929\n",
      "Step: 299000, Epsilon: 0.3534899999941272, Loss: 0.529585063457489\n",
      "Step: 300000, Epsilon: 0.3529999999941076, Loss: 0.4739026129245758\n",
      "Step: 301000, Epsilon: 0.35250999999408794, Loss: 0.4181482195854187\n",
      "Step: 302000, Epsilon: 0.3520199999940683, Loss: 0.48693299293518066\n",
      "Step: 303000, Epsilon: 0.35152999999404866, Loss: 0.535141110420227\n",
      "Step: 304000, Epsilon: 0.351039999994029, Loss: 0.5913944244384766\n",
      "Step: 305000, Epsilon: 0.3505499999940094, Loss: 0.4318796694278717\n",
      "Step: 306000, Epsilon: 0.35005999999398973, Loss: 0.6063902974128723\n",
      "Step: 307000, Epsilon: 0.3495699999939701, Loss: 0.4196029305458069\n",
      "Step: 308000, Epsilon: 0.34907999999395045, Loss: 0.5058614015579224\n",
      "Step: 309000, Epsilon: 0.3485899999939308, Loss: 0.43319475650787354\n",
      "Step: 310000, Epsilon: 0.34809999999391117, Loss: 0.522676944732666\n",
      "Step: 311000, Epsilon: 0.3476099999938915, Loss: 0.43889763951301575\n",
      "Step: 312000, Epsilon: 0.3471199999938719, Loss: 0.4445773959159851\n",
      "Step: 313000, Epsilon: 0.34662999999385224, Loss: 0.5669065117835999\n",
      "Step: 314000, Epsilon: 0.3461399999938326, Loss: 0.5372807383537292\n",
      "Step: 315000, Epsilon: 0.34564999999381296, Loss: 0.5118149518966675\n",
      "Step: 316000, Epsilon: 0.3451599999937933, Loss: 0.5811319351196289\n",
      "Step: 317000, Epsilon: 0.3446699999937737, Loss: 0.5278626680374146\n",
      "Step: 318000, Epsilon: 0.34417999999375404, Loss: 0.46382588148117065\n",
      "Step: 319000, Epsilon: 0.3436899999937344, Loss: 0.49804940819740295\n",
      "Step: 320000, Epsilon: 0.34319999999371475, Loss: 0.6176488399505615\n",
      "Step: 321000, Epsilon: 0.3427099999936951, Loss: 0.506323516368866\n",
      "Step: 322000, Epsilon: 0.34221999999367547, Loss: 0.5665918588638306\n",
      "Step: 323000, Epsilon: 0.34172999999365583, Loss: 0.5656117796897888\n",
      "Step: 324000, Epsilon: 0.3412399999936362, Loss: 0.47785457968711853\n",
      "Step: 325000, Epsilon: 0.34074999999361655, Loss: 0.4408112168312073\n",
      "Step: 326000, Epsilon: 0.3402599999935969, Loss: 0.49052658677101135\n",
      "Step: 327000, Epsilon: 0.33976999999357727, Loss: 0.6789880394935608\n",
      "Step: 328000, Epsilon: 0.3392799999935576, Loss: 0.5756102204322815\n",
      "Step: 329000, Epsilon: 0.338789999993538, Loss: 0.502968966960907\n",
      "Step: 330000, Epsilon: 0.33829999999351834, Loss: 0.4526621103286743\n",
      "Step: 331000, Epsilon: 0.3378099999934987, Loss: 0.4925532639026642\n",
      "Step: 332000, Epsilon: 0.33731999999347906, Loss: 0.5568996667861938\n",
      "Step: 333000, Epsilon: 0.3368299999934594, Loss: 0.4852716624736786\n",
      "Step: 334000, Epsilon: 0.3363399999934398, Loss: 0.46684056520462036\n",
      "Step: 335000, Epsilon: 0.33584999999342013, Loss: 0.42321550846099854\n",
      "Step: 336000, Epsilon: 0.3353599999934005, Loss: 0.4704570174217224\n",
      "Step: 337000, Epsilon: 0.33486999999338085, Loss: 0.4765186309814453\n",
      "Step: 338000, Epsilon: 0.3343799999933612, Loss: 0.5169885158538818\n",
      "Step: 339000, Epsilon: 0.33388999999334157, Loss: 0.5746535062789917\n",
      "Step: 340000, Epsilon: 0.3333999999933219, Loss: 0.6144887804985046\n",
      "Step: 341000, Epsilon: 0.3329099999933023, Loss: 0.5581897497177124\n",
      "Step: 342000, Epsilon: 0.33241999999328264, Loss: 0.5075146555900574\n",
      "Step: 343000, Epsilon: 0.331929999993263, Loss: 0.4914247989654541\n",
      "Step: 344000, Epsilon: 0.33143999999324336, Loss: 0.5760050415992737\n",
      "Step: 345000, Epsilon: 0.3309499999932237, Loss: 0.4477774202823639\n",
      "Step: 346000, Epsilon: 0.3304599999932041, Loss: 0.47923386096954346\n",
      "Step: 347000, Epsilon: 0.32996999999318444, Loss: 0.6335070133209229\n",
      "Step: 348000, Epsilon: 0.3294799999931648, Loss: 0.4628887474536896\n",
      "Step: 349000, Epsilon: 0.32898999999314515, Loss: 0.571771502494812\n",
      "Step: 350000, Epsilon: 0.3284999999931255, Loss: 0.4394141137599945\n",
      "Step: 351000, Epsilon: 0.32800999999310587, Loss: 0.49571356177330017\n",
      "Step: 352000, Epsilon: 0.32751999999308623, Loss: 0.5007031559944153\n",
      "Step: 353000, Epsilon: 0.3270299999930666, Loss: 0.4304164946079254\n",
      "Step: 354000, Epsilon: 0.32653999999304695, Loss: 0.49204221367836\n",
      "Step: 355000, Epsilon: 0.3260499999930273, Loss: 0.43615397810935974\n",
      "Step: 356000, Epsilon: 0.32555999999300766, Loss: 0.39674827456474304\n",
      "Step: 357000, Epsilon: 0.325069999992988, Loss: 0.37394076585769653\n",
      "Step: 358000, Epsilon: 0.3245799999929684, Loss: 0.4282822012901306\n",
      "Step: 359000, Epsilon: 0.32408999999294874, Loss: 0.45601287484169006\n",
      "Step: 360000, Epsilon: 0.3235999999929291, Loss: 0.4646280109882355\n",
      "Step: 361000, Epsilon: 0.32310999999290946, Loss: 0.5074543952941895\n",
      "Step: 362000, Epsilon: 0.3226199999928898, Loss: 0.4008563756942749\n",
      "Step: 363000, Epsilon: 0.3221299999928702, Loss: 0.396567165851593\n",
      "Step: 364000, Epsilon: 0.32163999999285053, Loss: 0.48240798711776733\n",
      "Step: 365000, Epsilon: 0.3211499999928309, Loss: 0.49105578660964966\n",
      "Step: 366000, Epsilon: 0.32065999999281125, Loss: 0.36212119460105896\n",
      "Step: 367000, Epsilon: 0.3201699999927916, Loss: 0.40117859840393066\n",
      "Step: 368000, Epsilon: 0.31967999999277197, Loss: 0.3801015615463257\n",
      "Step: 369000, Epsilon: 0.3191899999927523, Loss: 0.40823665261268616\n",
      "Step: 370000, Epsilon: 0.3186999999927327, Loss: 0.38067036867141724\n",
      "Step: 371000, Epsilon: 0.31820999999271304, Loss: 0.4066624641418457\n",
      "Step: 372000, Epsilon: 0.3177199999926934, Loss: 0.31988757848739624\n",
      "Step: 373000, Epsilon: 0.31722999999267376, Loss: 0.39863884449005127\n",
      "Step: 374000, Epsilon: 0.3167399999926541, Loss: 0.37551149725914\n",
      "Step: 375000, Epsilon: 0.3162499999926345, Loss: 0.36552780866622925\n",
      "Step: 376000, Epsilon: 0.31575999999261484, Loss: 0.3583531081676483\n",
      "Step: 377000, Epsilon: 0.3152699999925952, Loss: 0.30141738057136536\n",
      "Step: 378000, Epsilon: 0.31477999999257555, Loss: 0.4302460849285126\n",
      "Step: 379000, Epsilon: 0.3142899999925559, Loss: 0.34339138865470886\n",
      "Step: 380000, Epsilon: 0.31379999999253627, Loss: 0.34095701575279236\n",
      "Step: 381000, Epsilon: 0.31330999999251663, Loss: 0.27281031012535095\n",
      "Step: 382000, Epsilon: 0.312819999992497, Loss: 0.37908148765563965\n",
      "Step: 383000, Epsilon: 0.31232999999247735, Loss: 0.3128356635570526\n",
      "Step: 384000, Epsilon: 0.3118399999924577, Loss: 0.26683127880096436\n",
      "Step: 385000, Epsilon: 0.31134999999243806, Loss: 0.3571942150592804\n",
      "Step: 386000, Epsilon: 0.3108599999924184, Loss: 0.3456401824951172\n",
      "Step: 387000, Epsilon: 0.3103699999923988, Loss: 0.3489493131637573\n",
      "Step: 388000, Epsilon: 0.30987999999237914, Loss: 0.25786033272743225\n",
      "Step: 389000, Epsilon: 0.3093899999923595, Loss: 0.37659627199172974\n",
      "Step: 390000, Epsilon: 0.30889999999233986, Loss: 0.3504476845264435\n",
      "Step: 391000, Epsilon: 0.3084099999923202, Loss: 0.38525164127349854\n",
      "Step: 392000, Epsilon: 0.3079199999923006, Loss: 0.30755093693733215\n",
      "Step: 393000, Epsilon: 0.30742999999228093, Loss: 0.4316994547843933\n",
      "Step: 394000, Epsilon: 0.3069399999922613, Loss: 0.3820233643054962\n",
      "Step: 395000, Epsilon: 0.30644999999224165, Loss: 0.40774592757225037\n",
      "Step: 396000, Epsilon: 0.305959999992222, Loss: 0.4218226969242096\n",
      "Step: 397000, Epsilon: 0.30546999999220237, Loss: 0.3985937833786011\n",
      "Step: 398000, Epsilon: 0.3049799999921827, Loss: 0.4001893401145935\n",
      "Step: 399000, Epsilon: 0.3044899999921631, Loss: 0.3387727737426758\n",
      "Step: 400000, Epsilon: 0.30399999999214344, Loss: 0.40671655535697937\n",
      "Step: 401000, Epsilon: 0.3035099999921238, Loss: 0.3575074076652527\n",
      "Step: 402000, Epsilon: 0.30301999999210416, Loss: 0.3726186156272888\n",
      "Step: 403000, Epsilon: 0.3025299999920845, Loss: 0.2986290454864502\n",
      "Step: 404000, Epsilon: 0.3020399999920649, Loss: 0.28744959831237793\n",
      "Step: 405000, Epsilon: 0.30154999999204524, Loss: 0.3662334978580475\n",
      "Step: 406000, Epsilon: 0.3010599999920256, Loss: 0.4162297248840332\n",
      "Step: 407000, Epsilon: 0.30056999999200595, Loss: 0.426318883895874\n",
      "Step: 408000, Epsilon: 0.3000799999919863, Loss: 0.4678082764148712\n",
      "Step: 409000, Epsilon: 0.29958999999196667, Loss: 0.46085041761398315\n",
      "Step: 410000, Epsilon: 0.29909999999194703, Loss: 0.3837243914604187\n",
      "Step: 411000, Epsilon: 0.2986099999919274, Loss: 0.3718234896659851\n",
      "Step: 412000, Epsilon: 0.29811999999190775, Loss: 0.4038802981376648\n",
      "Step: 413000, Epsilon: 0.2976299999918881, Loss: 0.2900323271751404\n",
      "Step: 414000, Epsilon: 0.29713999999186846, Loss: 0.4282960891723633\n",
      "Step: 415000, Epsilon: 0.2966499999918488, Loss: 0.3405846059322357\n",
      "Step: 416000, Epsilon: 0.2961599999918292, Loss: 0.31311577558517456\n",
      "Step: 417000, Epsilon: 0.29566999999180954, Loss: 0.43851345777511597\n",
      "Step: 418000, Epsilon: 0.2951799999917899, Loss: 0.3436540961265564\n",
      "Step: 419000, Epsilon: 0.29468999999177026, Loss: 0.36821573972702026\n",
      "Step: 420000, Epsilon: 0.2941999999917506, Loss: 0.429171621799469\n",
      "Step: 421000, Epsilon: 0.293709999991731, Loss: 0.4238379895687103\n",
      "Step: 422000, Epsilon: 0.29321999999171133, Loss: 0.40886372327804565\n",
      "Step: 423000, Epsilon: 0.2927299999916917, Loss: 0.3138192296028137\n",
      "Step: 424000, Epsilon: 0.29223999999167205, Loss: 0.3539530634880066\n",
      "Step: 425000, Epsilon: 0.2917499999916524, Loss: 0.32068538665771484\n",
      "Step: 426000, Epsilon: 0.29125999999163277, Loss: 0.3517668843269348\n",
      "Step: 427000, Epsilon: 0.2907699999916131, Loss: 0.3497365415096283\n",
      "Step: 428000, Epsilon: 0.2902799999915935, Loss: 0.287911981344223\n",
      "Step: 429000, Epsilon: 0.28978999999157384, Loss: 0.3145679235458374\n",
      "Step: 430000, Epsilon: 0.2892999999915542, Loss: 0.3929690718650818\n",
      "Step: 431000, Epsilon: 0.28880999999153456, Loss: 0.3259918987751007\n",
      "Step: 432000, Epsilon: 0.2883199999915149, Loss: 0.4245615601539612\n",
      "Step: 433000, Epsilon: 0.2878299999914953, Loss: 0.3748803734779358\n",
      "Step: 434000, Epsilon: 0.28733999999147564, Loss: 0.3720264434814453\n",
      "Step: 435000, Epsilon: 0.286849999991456, Loss: 0.3126652240753174\n",
      "Step: 436000, Epsilon: 0.28635999999143635, Loss: 0.4246898293495178\n",
      "Step: 437000, Epsilon: 0.2858699999914167, Loss: 0.44300419092178345\n",
      "Step: 438000, Epsilon: 0.28537999999139707, Loss: 0.39767879247665405\n",
      "Step: 439000, Epsilon: 0.28488999999137743, Loss: 0.4881207048892975\n",
      "Step: 440000, Epsilon: 0.2843999999913578, Loss: 0.4082565903663635\n",
      "Step: 441000, Epsilon: 0.28390999999133815, Loss: 0.4050513505935669\n",
      "Step: 442000, Epsilon: 0.2834199999913185, Loss: 0.4928097426891327\n",
      "Step: 443000, Epsilon: 0.28292999999129886, Loss: 0.45639950037002563\n",
      "Step: 444000, Epsilon: 0.2824399999912792, Loss: 0.4801584780216217\n",
      "Step: 445000, Epsilon: 0.2819499999912596, Loss: 0.48244062066078186\n",
      "Step: 446000, Epsilon: 0.28145999999123994, Loss: 0.38049575686454773\n",
      "Step: 447000, Epsilon: 0.2809699999912203, Loss: 0.568936824798584\n",
      "Step: 448000, Epsilon: 0.28047999999120066, Loss: 0.3909187912940979\n",
      "Step: 449000, Epsilon: 0.279989999991181, Loss: 0.4278068244457245\n",
      "Step: 450000, Epsilon: 0.2794999999911614, Loss: 0.3556351959705353\n",
      "Step: 451000, Epsilon: 0.27900999999114173, Loss: 0.37176796793937683\n",
      "Step: 452000, Epsilon: 0.2785199999911221, Loss: 0.4211367964744568\n",
      "Step: 453000, Epsilon: 0.27802999999110245, Loss: 0.3818349540233612\n",
      "Step: 454000, Epsilon: 0.2775399999910828, Loss: 0.3810523748397827\n",
      "Step: 455000, Epsilon: 0.27704999999106317, Loss: 0.35446980595588684\n",
      "Step: 456000, Epsilon: 0.2765599999910435, Loss: 0.4605039060115814\n",
      "Step: 457000, Epsilon: 0.2760699999910239, Loss: 0.4196041524410248\n",
      "Step: 458000, Epsilon: 0.27557999999100424, Loss: 0.31490546464920044\n",
      "Step: 459000, Epsilon: 0.2750899999909846, Loss: 0.3168972432613373\n",
      "Step: 460000, Epsilon: 0.27459999999096496, Loss: 0.39831623435020447\n",
      "Step: 461000, Epsilon: 0.2741099999909453, Loss: 0.30786460638046265\n",
      "Step: 462000, Epsilon: 0.2736199999909257, Loss: 0.3369656205177307\n",
      "Step: 463000, Epsilon: 0.27312999999090604, Loss: 0.35895153880119324\n",
      "Step: 464000, Epsilon: 0.2726399999908864, Loss: 0.3564300835132599\n",
      "Step: 465000, Epsilon: 0.27214999999086675, Loss: 0.36099472641944885\n",
      "Step: 466000, Epsilon: 0.2716599999908471, Loss: 0.39091047644615173\n",
      "Step: 467000, Epsilon: 0.27116999999082747, Loss: 0.42238181829452515\n",
      "Step: 468000, Epsilon: 0.27067999999080783, Loss: 0.3688027262687683\n",
      "Step: 469000, Epsilon: 0.2701899999907882, Loss: 0.42451590299606323\n",
      "Step: 470000, Epsilon: 0.26969999999076855, Loss: 0.38681504130363464\n",
      "Step: 471000, Epsilon: 0.2692099999907489, Loss: 0.39987343549728394\n",
      "Step: 472000, Epsilon: 0.26871999999072926, Loss: 0.3878695070743561\n",
      "Step: 473000, Epsilon: 0.2682299999907096, Loss: 0.3943609893321991\n",
      "Step: 474000, Epsilon: 0.26773999999069, Loss: 0.36488455533981323\n",
      "Step: 475000, Epsilon: 0.26724999999067034, Loss: 0.436050146818161\n",
      "Step: 476000, Epsilon: 0.2667599999906507, Loss: 0.4438682496547699\n",
      "Step: 477000, Epsilon: 0.26626999999063106, Loss: 0.34533369541168213\n",
      "Step: 478000, Epsilon: 0.2657799999906114, Loss: 0.3902624845504761\n",
      "Step: 479000, Epsilon: 0.2652899999905918, Loss: 0.4016207456588745\n",
      "Step: 480000, Epsilon: 0.26479999999057213, Loss: 0.38187915086746216\n",
      "Step: 481000, Epsilon: 0.2643099999905525, Loss: 0.28829196095466614\n",
      "Step: 482000, Epsilon: 0.26381999999053285, Loss: 0.3876449465751648\n",
      "Step: 483000, Epsilon: 0.2633299999905132, Loss: 0.372275173664093\n",
      "Step: 484000, Epsilon: 0.26283999999049357, Loss: 0.44774872064590454\n",
      "Step: 485000, Epsilon: 0.2623499999904739, Loss: 0.35773566365242004\n",
      "Step: 486000, Epsilon: 0.2618599999904543, Loss: 0.496108740568161\n",
      "Step: 487000, Epsilon: 0.26136999999043464, Loss: 0.38915473222732544\n",
      "Step: 488000, Epsilon: 0.260879999990415, Loss: 0.439620703458786\n",
      "Step: 489000, Epsilon: 0.26038999999039536, Loss: 0.48393476009368896\n",
      "Step: 490000, Epsilon: 0.2598999999903757, Loss: 0.4679774343967438\n",
      "Step: 491000, Epsilon: 0.2594099999903561, Loss: 0.3859023451805115\n",
      "Step: 492000, Epsilon: 0.25891999999033644, Loss: 0.44348886609077454\n",
      "Step: 493000, Epsilon: 0.2584299999903168, Loss: 0.38560354709625244\n",
      "Step: 494000, Epsilon: 0.25793999999029715, Loss: 0.44748491048812866\n",
      "Step: 495000, Epsilon: 0.2574499999902775, Loss: 0.3683088719844818\n",
      "Step: 496000, Epsilon: 0.25695999999025787, Loss: 0.3699791133403778\n",
      "Step: 497000, Epsilon: 0.25646999999023823, Loss: 0.4087375998497009\n",
      "Step: 498000, Epsilon: 0.2559799999902186, Loss: 0.40306439995765686\n",
      "Step: 499000, Epsilon: 0.25548999999019895, Loss: 0.3873746395111084\n",
      "Step: 500000, Epsilon: 0.2549999999901793, Loss: 0.4533170163631439\n",
      "Step: 501000, Epsilon: 0.25450999999015966, Loss: 0.34332358837127686\n",
      "Step: 502000, Epsilon: 0.25401999999014, Loss: 0.47309941053390503\n",
      "Step: 503000, Epsilon: 0.2535299999901204, Loss: 0.40367719531059265\n",
      "Step: 504000, Epsilon: 0.25303999999010074, Loss: 0.45782342553138733\n",
      "Step: 505000, Epsilon: 0.2525499999900811, Loss: 0.392846941947937\n",
      "Step: 506000, Epsilon: 0.25205999999006146, Loss: 0.3922063410282135\n",
      "Step: 507000, Epsilon: 0.2515699999900418, Loss: 0.3426242172718048\n",
      "Step: 508000, Epsilon: 0.2510799999900222, Loss: 0.4353780150413513\n",
      "Step: 509000, Epsilon: 0.25058999999000253, Loss: 0.4189609885215759\n",
      "Step: 510000, Epsilon: 0.2500999999899829, Loss: 0.4951527714729309\n",
      "Step: 511000, Epsilon: 0.24960999998998534, Loss: 0.33187562227249146\n",
      "Step: 512000, Epsilon: 0.24911999998999346, Loss: 0.5203062295913696\n",
      "Step: 513000, Epsilon: 0.24862999999000157, Loss: 0.41131746768951416\n",
      "Step: 514000, Epsilon: 0.24813999999000969, Loss: 0.4050399661064148\n",
      "Step: 515000, Epsilon: 0.2476499999900178, Loss: 0.46177610754966736\n",
      "Step: 516000, Epsilon: 0.2471599999900259, Loss: 0.42086946964263916\n",
      "Step: 517000, Epsilon: 0.24666999999003403, Loss: 0.5177467465400696\n",
      "Step: 518000, Epsilon: 0.24617999999004214, Loss: 0.4258020520210266\n",
      "Step: 519000, Epsilon: 0.24568999999005026, Loss: 0.3111007511615753\n",
      "Step: 520000, Epsilon: 0.24519999999005837, Loss: 0.4090273678302765\n",
      "Step: 521000, Epsilon: 0.24470999999006648, Loss: 0.4561643898487091\n",
      "Step: 522000, Epsilon: 0.2442199999900746, Loss: 0.39959490299224854\n",
      "Step: 523000, Epsilon: 0.2437299999900827, Loss: 0.46542805433273315\n",
      "Step: 524000, Epsilon: 0.24323999999009083, Loss: 0.3700191080570221\n",
      "Step: 525000, Epsilon: 0.24274999999009894, Loss: 0.389849454164505\n",
      "Step: 526000, Epsilon: 0.24225999999010706, Loss: 0.4134038984775543\n",
      "Step: 527000, Epsilon: 0.24176999999011517, Loss: 0.32452648878097534\n",
      "Step: 528000, Epsilon: 0.24127999999012328, Loss: 0.3465687930583954\n",
      "Step: 529000, Epsilon: 0.2407899999901314, Loss: 0.4474007189273834\n",
      "Step: 530000, Epsilon: 0.2402999999901395, Loss: 0.37109389901161194\n",
      "Step: 531000, Epsilon: 0.23980999999014763, Loss: 0.3734986186027527\n",
      "Step: 532000, Epsilon: 0.23931999999015574, Loss: 0.3427448570728302\n",
      "Step: 533000, Epsilon: 0.23882999999016385, Loss: 0.29362234473228455\n",
      "Step: 534000, Epsilon: 0.23833999999017197, Loss: 0.27020373940467834\n",
      "Step: 535000, Epsilon: 0.23784999999018008, Loss: 0.4050208032131195\n",
      "Step: 536000, Epsilon: 0.2373599999901882, Loss: 0.36467838287353516\n",
      "Step: 537000, Epsilon: 0.2368699999901963, Loss: 0.2663430869579315\n",
      "Step: 538000, Epsilon: 0.23637999999020443, Loss: 0.3437216579914093\n",
      "Step: 539000, Epsilon: 0.23588999999021254, Loss: 0.5218525528907776\n",
      "Step: 540000, Epsilon: 0.23539999999022065, Loss: 0.4358344078063965\n",
      "Step: 541000, Epsilon: 0.23490999999022877, Loss: 0.46871086955070496\n",
      "Step: 542000, Epsilon: 0.23441999999023688, Loss: 0.41716068983078003\n",
      "Step: 543000, Epsilon: 0.233929999990245, Loss: 0.40179917216300964\n",
      "Step: 544000, Epsilon: 0.2334399999902531, Loss: 0.3795984387397766\n",
      "Step: 545000, Epsilon: 0.23294999999026123, Loss: 0.3994012773036957\n",
      "Step: 546000, Epsilon: 0.23245999999026934, Loss: 0.470814049243927\n",
      "Step: 547000, Epsilon: 0.23196999999027745, Loss: 0.4812186360359192\n",
      "Step: 548000, Epsilon: 0.23147999999028557, Loss: 0.4547973573207855\n",
      "Step: 549000, Epsilon: 0.23098999999029368, Loss: 0.3950575888156891\n",
      "Step: 550000, Epsilon: 0.2304999999903018, Loss: 0.38270246982574463\n",
      "Step: 551000, Epsilon: 0.2300099999903099, Loss: 0.4711064398288727\n",
      "Step: 552000, Epsilon: 0.22951999999031802, Loss: 0.42624324560165405\n",
      "Step: 553000, Epsilon: 0.22902999999032614, Loss: 0.43078187108039856\n",
      "Step: 554000, Epsilon: 0.22853999999033425, Loss: 0.41730427742004395\n",
      "Step: 555000, Epsilon: 0.22804999999034237, Loss: 0.49202626943588257\n",
      "Step: 556000, Epsilon: 0.22755999999035048, Loss: 0.48143720626831055\n",
      "Step: 557000, Epsilon: 0.2270699999903586, Loss: 0.5208793878555298\n",
      "Step: 558000, Epsilon: 0.2265799999903667, Loss: 0.4068640172481537\n",
      "Step: 559000, Epsilon: 0.22608999999037482, Loss: 0.6335179805755615\n",
      "Step: 560000, Epsilon: 0.22559999999038294, Loss: 0.49697068333625793\n",
      "Step: 561000, Epsilon: 0.22510999999039105, Loss: 0.4678921699523926\n",
      "Step: 562000, Epsilon: 0.22461999999039917, Loss: 0.40627434849739075\n",
      "Step: 563000, Epsilon: 0.22412999999040728, Loss: 0.49863049387931824\n",
      "Step: 564000, Epsilon: 0.2236399999904154, Loss: 0.4912492632865906\n",
      "Step: 565000, Epsilon: 0.2231499999904235, Loss: 0.47352850437164307\n",
      "Step: 566000, Epsilon: 0.22265999999043162, Loss: 0.4272975027561188\n",
      "Step: 567000, Epsilon: 0.22216999999043974, Loss: 0.47044265270233154\n",
      "Step: 568000, Epsilon: 0.22167999999044785, Loss: 0.42969533801078796\n",
      "Step: 569000, Epsilon: 0.22118999999045597, Loss: 0.43454188108444214\n",
      "Step: 570000, Epsilon: 0.22069999999046408, Loss: 0.36944320797920227\n",
      "Step: 571000, Epsilon: 0.2202099999904722, Loss: 0.4903791844844818\n",
      "Step: 572000, Epsilon: 0.2197199999904803, Loss: 0.4406571090221405\n",
      "Step: 573000, Epsilon: 0.21922999999048842, Loss: 0.4880337715148926\n",
      "Step: 574000, Epsilon: 0.21873999999049654, Loss: 0.47097283601760864\n",
      "Step: 575000, Epsilon: 0.21824999999050465, Loss: 0.37637439370155334\n",
      "Step: 576000, Epsilon: 0.21775999999051276, Loss: 0.32357269525527954\n",
      "Step: 577000, Epsilon: 0.21726999999052088, Loss: 0.4161777198314667\n",
      "Step: 578000, Epsilon: 0.216779999990529, Loss: 0.31861555576324463\n",
      "Step: 579000, Epsilon: 0.2162899999905371, Loss: 0.478841096162796\n",
      "Step: 580000, Epsilon: 0.21579999999054522, Loss: 0.36890289187431335\n",
      "Step: 581000, Epsilon: 0.21530999999055334, Loss: 0.4446069300174713\n",
      "Step: 582000, Epsilon: 0.21481999999056145, Loss: 0.3404574990272522\n",
      "Step: 583000, Epsilon: 0.21432999999056956, Loss: 0.45225974917411804\n",
      "Step: 584000, Epsilon: 0.21383999999057768, Loss: 0.35678061842918396\n",
      "Step: 585000, Epsilon: 0.2133499999905858, Loss: 0.4082043468952179\n",
      "Step: 586000, Epsilon: 0.2128599999905939, Loss: 0.40036529302597046\n",
      "Step: 587000, Epsilon: 0.21236999999060202, Loss: 0.3277107775211334\n",
      "Step: 588000, Epsilon: 0.21187999999061014, Loss: 0.4345192313194275\n",
      "Step: 589000, Epsilon: 0.21138999999061825, Loss: 0.39325326681137085\n",
      "Step: 590000, Epsilon: 0.21089999999062636, Loss: 0.5013993382453918\n",
      "Step: 591000, Epsilon: 0.21040999999063448, Loss: 0.3600843548774719\n",
      "Step: 592000, Epsilon: 0.2099199999906426, Loss: 0.393327534198761\n",
      "Step: 593000, Epsilon: 0.2094299999906507, Loss: 0.43352752923965454\n",
      "Step: 594000, Epsilon: 0.20893999999065882, Loss: 0.3371667265892029\n",
      "Step: 595000, Epsilon: 0.20844999999066693, Loss: 0.43311771750450134\n",
      "Step: 596000, Epsilon: 0.20795999999067505, Loss: 0.42911481857299805\n",
      "Step: 597000, Epsilon: 0.20746999999068316, Loss: 0.399413526058197\n",
      "Step: 598000, Epsilon: 0.20697999999069128, Loss: 0.4258216619491577\n",
      "Step: 599000, Epsilon: 0.2064899999906994, Loss: 0.38223007321357727\n",
      "Step: 600000, Epsilon: 0.2059999999907075, Loss: 0.39778637886047363\n",
      "Step: 601000, Epsilon: 0.20550999999071562, Loss: 0.40166547894477844\n",
      "Step: 602000, Epsilon: 0.20501999999072373, Loss: 0.33629903197288513\n",
      "Step: 603000, Epsilon: 0.20452999999073185, Loss: 0.4285787343978882\n",
      "Step: 604000, Epsilon: 0.20403999999073996, Loss: 0.4633440375328064\n",
      "Step: 605000, Epsilon: 0.20354999999074808, Loss: 0.4758077561855316\n",
      "Step: 606000, Epsilon: 0.2030599999907562, Loss: 0.32948756217956543\n",
      "Step: 607000, Epsilon: 0.2025699999907643, Loss: 0.3886486887931824\n",
      "Step: 608000, Epsilon: 0.20207999999077242, Loss: 0.36096638441085815\n",
      "Step: 609000, Epsilon: 0.20158999999078053, Loss: 0.3795803487300873\n",
      "Step: 610000, Epsilon: 0.20109999999078865, Loss: 0.3071575462818146\n",
      "Step: 611000, Epsilon: 0.20060999999079676, Loss: 0.428621381521225\n",
      "Step: 612000, Epsilon: 0.20011999999080488, Loss: 0.27603840827941895\n",
      "Step: 613000, Epsilon: 0.199629999990813, Loss: 0.33240965008735657\n",
      "Step: 614000, Epsilon: 0.1991399999908211, Loss: 0.34805259108543396\n",
      "Step: 615000, Epsilon: 0.19864999999082922, Loss: 0.2876376509666443\n",
      "Step: 616000, Epsilon: 0.19815999999083733, Loss: 0.326582670211792\n",
      "Step: 617000, Epsilon: 0.19766999999084545, Loss: 0.3469507694244385\n",
      "Step: 618000, Epsilon: 0.19717999999085356, Loss: 0.33847948908805847\n",
      "Step: 619000, Epsilon: 0.19668999999086167, Loss: 0.25027841329574585\n",
      "Step: 620000, Epsilon: 0.1961999999908698, Loss: 0.35569632053375244\n",
      "Step: 621000, Epsilon: 0.1957099999908779, Loss: 0.3851223587989807\n",
      "Step: 622000, Epsilon: 0.19521999999088602, Loss: 0.2829751968383789\n",
      "Step: 623000, Epsilon: 0.19472999999089413, Loss: 0.365692675113678\n",
      "Step: 624000, Epsilon: 0.19423999999090225, Loss: 0.28039032220840454\n",
      "Step: 625000, Epsilon: 0.19374999999091036, Loss: 0.33883994817733765\n",
      "Step: 626000, Epsilon: 0.19325999999091847, Loss: 0.3014222979545593\n",
      "Step: 627000, Epsilon: 0.1927699999909266, Loss: 0.35676199197769165\n",
      "Step: 628000, Epsilon: 0.1922799999909347, Loss: 0.29108569025993347\n",
      "Step: 629000, Epsilon: 0.19178999999094282, Loss: 0.32855817675590515\n",
      "Step: 630000, Epsilon: 0.19129999999095093, Loss: 0.3262740671634674\n",
      "Step: 631000, Epsilon: 0.19080999999095904, Loss: 0.30751127004623413\n",
      "Step: 632000, Epsilon: 0.19031999999096716, Loss: 0.3448408246040344\n",
      "Step: 633000, Epsilon: 0.18982999999097527, Loss: 0.2961496412754059\n",
      "Step: 634000, Epsilon: 0.1893399999909834, Loss: 0.3144874572753906\n",
      "Step: 635000, Epsilon: 0.1888499999909915, Loss: 0.2993091642856598\n",
      "Step: 636000, Epsilon: 0.18835999999099962, Loss: 0.30113014578819275\n",
      "Step: 637000, Epsilon: 0.18786999999100773, Loss: 0.31462618708610535\n",
      "Step: 638000, Epsilon: 0.18737999999101584, Loss: 0.27875879406929016\n",
      "Step: 639000, Epsilon: 0.18688999999102396, Loss: 0.3401286005973816\n",
      "Step: 640000, Epsilon: 0.18639999999103207, Loss: 0.3486860692501068\n",
      "Step: 641000, Epsilon: 0.1859099999910402, Loss: 0.3090052902698517\n",
      "Step: 642000, Epsilon: 0.1854199999910483, Loss: 0.3120090365409851\n",
      "Step: 643000, Epsilon: 0.18492999999105642, Loss: 0.3234189748764038\n",
      "Step: 644000, Epsilon: 0.18443999999106453, Loss: 0.3291774094104767\n",
      "Step: 645000, Epsilon: 0.18394999999107264, Loss: 0.3548397421836853\n",
      "Step: 646000, Epsilon: 0.18345999999108076, Loss: 0.35856327414512634\n",
      "Step: 647000, Epsilon: 0.18296999999108887, Loss: 0.3097507655620575\n",
      "Step: 648000, Epsilon: 0.18247999999109699, Loss: 0.43963417410850525\n",
      "Step: 649000, Epsilon: 0.1819899999911051, Loss: 0.3086967468261719\n",
      "Step: 650000, Epsilon: 0.18149999999111321, Loss: 0.2948971092700958\n",
      "Step: 651000, Epsilon: 0.18100999999112133, Loss: 0.3537024259567261\n",
      "Step: 652000, Epsilon: 0.18051999999112944, Loss: 0.36339643597602844\n",
      "Step: 653000, Epsilon: 0.18002999999113756, Loss: 0.28103265166282654\n",
      "Step: 654000, Epsilon: 0.17953999999114567, Loss: 0.318392813205719\n",
      "Step: 655000, Epsilon: 0.17904999999115379, Loss: 0.3550781011581421\n",
      "Step: 656000, Epsilon: 0.1785599999911619, Loss: 0.3522653579711914\n",
      "Step: 657000, Epsilon: 0.17806999999117, Loss: 0.3528931438922882\n",
      "Step: 658000, Epsilon: 0.17757999999117813, Loss: 0.384448766708374\n",
      "Step: 659000, Epsilon: 0.17708999999118624, Loss: 0.36467865109443665\n",
      "Step: 660000, Epsilon: 0.17659999999119436, Loss: 0.44416648149490356\n",
      "Step: 661000, Epsilon: 0.17610999999120247, Loss: 0.3043683171272278\n",
      "Step: 662000, Epsilon: 0.17561999999121058, Loss: 0.3543391823768616\n",
      "Step: 663000, Epsilon: 0.1751299999912187, Loss: 0.3908008933067322\n",
      "Step: 664000, Epsilon: 0.1746399999912268, Loss: 0.40813007950782776\n",
      "Step: 665000, Epsilon: 0.17414999999123493, Loss: 0.37234511971473694\n",
      "Step: 666000, Epsilon: 0.17365999999124304, Loss: 0.4561459422111511\n",
      "Step: 667000, Epsilon: 0.17316999999125116, Loss: 0.38765037059783936\n",
      "Step: 668000, Epsilon: 0.17267999999125927, Loss: 0.4717644453048706\n",
      "Step: 669000, Epsilon: 0.17218999999126738, Loss: 0.4543532729148865\n",
      "Step: 670000, Epsilon: 0.1716999999912755, Loss: 0.47890228033065796\n",
      "Step: 671000, Epsilon: 0.1712099999912836, Loss: 0.4259973168373108\n",
      "Step: 672000, Epsilon: 0.17071999999129173, Loss: 0.2413169890642166\n",
      "Step: 673000, Epsilon: 0.17022999999129984, Loss: 0.44954046607017517\n",
      "Step: 674000, Epsilon: 0.16973999999130795, Loss: 0.40514251589775085\n",
      "Step: 675000, Epsilon: 0.16924999999131607, Loss: 0.39443260431289673\n",
      "Step: 676000, Epsilon: 0.16875999999132418, Loss: 0.4254397451877594\n",
      "Step: 677000, Epsilon: 0.1682699999913323, Loss: 0.4197573661804199\n",
      "Step: 678000, Epsilon: 0.1677799999913404, Loss: 0.4007885456085205\n",
      "Step: 679000, Epsilon: 0.16728999999134853, Loss: 0.4044126868247986\n",
      "Step: 680000, Epsilon: 0.16679999999135664, Loss: 0.3831448256969452\n",
      "Step: 681000, Epsilon: 0.16630999999136475, Loss: 0.32848960161209106\n",
      "Step: 682000, Epsilon: 0.16581999999137287, Loss: 0.4064250886440277\n",
      "Step: 683000, Epsilon: 0.16532999999138098, Loss: 0.38665756583213806\n",
      "Step: 684000, Epsilon: 0.1648399999913891, Loss: 0.36737966537475586\n",
      "Step: 685000, Epsilon: 0.1643499999913972, Loss: 0.3957230746746063\n",
      "Step: 686000, Epsilon: 0.16385999999140533, Loss: 0.38751140236854553\n",
      "Step: 687000, Epsilon: 0.16336999999141344, Loss: 0.4016238749027252\n",
      "Step: 688000, Epsilon: 0.16287999999142155, Loss: 0.3690573573112488\n",
      "Step: 689000, Epsilon: 0.16238999999142967, Loss: 0.36684486269950867\n",
      "Step: 690000, Epsilon: 0.16189999999143778, Loss: 0.4050387144088745\n",
      "Step: 691000, Epsilon: 0.1614099999914459, Loss: 0.3957836329936981\n",
      "Step: 692000, Epsilon: 0.160919999991454, Loss: 0.4186263680458069\n",
      "Step: 693000, Epsilon: 0.16042999999146212, Loss: 0.38424426317214966\n",
      "Step: 694000, Epsilon: 0.15993999999147024, Loss: 0.43069255352020264\n",
      "Step: 695000, Epsilon: 0.15944999999147835, Loss: 0.38859251141548157\n",
      "Step: 696000, Epsilon: 0.15895999999148647, Loss: 0.2890833914279938\n",
      "Step: 697000, Epsilon: 0.15846999999149458, Loss: 0.29794782400131226\n",
      "Step: 698000, Epsilon: 0.1579799999915027, Loss: 0.4027821719646454\n",
      "Step: 699000, Epsilon: 0.1574899999915108, Loss: 0.2215992659330368\n",
      "Step: 700000, Epsilon: 0.15699999999151892, Loss: 0.39956915378570557\n",
      "Step: 701000, Epsilon: 0.15650999999152704, Loss: 0.3653307557106018\n",
      "Step: 702000, Epsilon: 0.15601999999153515, Loss: 0.3282856345176697\n",
      "Step: 703000, Epsilon: 0.15552999999154327, Loss: 0.3277931213378906\n",
      "Step: 704000, Epsilon: 0.15503999999155138, Loss: 0.3209872245788574\n",
      "Step: 705000, Epsilon: 0.1545499999915595, Loss: 0.3241434395313263\n",
      "Step: 706000, Epsilon: 0.1540599999915676, Loss: 0.2877453565597534\n",
      "Step: 707000, Epsilon: 0.15356999999157572, Loss: 0.31958845257759094\n",
      "Step: 708000, Epsilon: 0.15307999999158384, Loss: 0.26738646626472473\n",
      "Step: 709000, Epsilon: 0.15258999999159195, Loss: 0.32386770844459534\n",
      "Step: 710000, Epsilon: 0.15209999999160007, Loss: 0.33480438590049744\n",
      "Step: 711000, Epsilon: 0.15160999999160818, Loss: 0.2818525433540344\n",
      "Step: 712000, Epsilon: 0.1511199999916163, Loss: 0.2769621014595032\n",
      "Step: 713000, Epsilon: 0.1506299999916244, Loss: 0.30301132798194885\n",
      "Step: 714000, Epsilon: 0.15013999999163252, Loss: 0.24497707188129425\n",
      "Step: 715000, Epsilon: 0.14964999999164064, Loss: 0.33469584584236145\n",
      "Step: 716000, Epsilon: 0.14915999999164875, Loss: 0.3373645842075348\n",
      "Step: 717000, Epsilon: 0.14866999999165686, Loss: 0.3745788037776947\n",
      "Step: 718000, Epsilon: 0.14817999999166498, Loss: 0.3149099051952362\n",
      "Step: 719000, Epsilon: 0.1476899999916731, Loss: 0.4080001413822174\n",
      "Step: 720000, Epsilon: 0.1471999999916812, Loss: 0.2875358462333679\n",
      "Step: 721000, Epsilon: 0.14670999999168932, Loss: 0.28248119354248047\n",
      "Step: 722000, Epsilon: 0.14621999999169744, Loss: 0.37325775623321533\n",
      "Step: 723000, Epsilon: 0.14572999999170555, Loss: 0.32209786772727966\n",
      "Step: 724000, Epsilon: 0.14523999999171366, Loss: 0.33326271176338196\n",
      "Step: 725000, Epsilon: 0.14474999999172178, Loss: 0.36495283246040344\n",
      "Step: 726000, Epsilon: 0.1442599999917299, Loss: 0.33943411707878113\n",
      "Step: 727000, Epsilon: 0.143769999991738, Loss: 0.3376457989215851\n",
      "Step: 728000, Epsilon: 0.14327999999174612, Loss: 0.32501310110092163\n",
      "Step: 729000, Epsilon: 0.14278999999175424, Loss: 0.2610454857349396\n",
      "Step: 730000, Epsilon: 0.14229999999176235, Loss: 0.23664018511772156\n",
      "Step: 731000, Epsilon: 0.14180999999177046, Loss: 0.26594701409339905\n",
      "Step: 732000, Epsilon: 0.14131999999177858, Loss: 0.34149423241615295\n",
      "Step: 733000, Epsilon: 0.1408299999917867, Loss: 0.34063389897346497\n",
      "Step: 734000, Epsilon: 0.1403399999917948, Loss: 0.262160062789917\n",
      "Step: 735000, Epsilon: 0.13984999999180292, Loss: 0.3126756250858307\n",
      "Step: 736000, Epsilon: 0.13935999999181103, Loss: 0.43920454382896423\n",
      "Step: 737000, Epsilon: 0.13886999999181915, Loss: 0.29143744707107544\n",
      "Step: 738000, Epsilon: 0.13837999999182726, Loss: 0.3186272978782654\n",
      "Step: 739000, Epsilon: 0.13788999999183538, Loss: 0.3030688762664795\n",
      "Step: 740000, Epsilon: 0.1373999999918435, Loss: 0.34484994411468506\n",
      "Step: 741000, Epsilon: 0.1369099999918516, Loss: 0.2520875334739685\n",
      "Step: 742000, Epsilon: 0.13641999999185972, Loss: 0.36491313576698303\n",
      "Step: 743000, Epsilon: 0.13592999999186783, Loss: 0.27035272121429443\n",
      "Step: 744000, Epsilon: 0.13543999999187595, Loss: 0.29479333758354187\n",
      "Step: 745000, Epsilon: 0.13494999999188406, Loss: 0.37495994567871094\n",
      "Step: 746000, Epsilon: 0.13445999999189218, Loss: 0.316848486661911\n",
      "Step: 747000, Epsilon: 0.1339699999919003, Loss: 0.24893984198570251\n",
      "Step: 748000, Epsilon: 0.1334799999919084, Loss: 0.3146496117115021\n",
      "Step: 749000, Epsilon: 0.13298999999191652, Loss: 0.3297433853149414\n",
      "Step: 750000, Epsilon: 0.13249999999192463, Loss: 0.2884887158870697\n",
      "Step: 751000, Epsilon: 0.13200999999193275, Loss: 0.29638969898223877\n",
      "Step: 752000, Epsilon: 0.13151999999194086, Loss: 0.2955068349838257\n",
      "Step: 753000, Epsilon: 0.13102999999194898, Loss: 0.35461732745170593\n",
      "Step: 754000, Epsilon: 0.1305399999919571, Loss: 0.28403669595718384\n",
      "Step: 755000, Epsilon: 0.1300499999919652, Loss: 0.39500483870506287\n",
      "Step: 756000, Epsilon: 0.12955999999197332, Loss: 0.27919575572013855\n",
      "Step: 757000, Epsilon: 0.12906999999198143, Loss: 0.334128201007843\n",
      "Step: 758000, Epsilon: 0.12857999999198955, Loss: 0.3063535690307617\n",
      "Step: 759000, Epsilon: 0.12808999999199766, Loss: 0.3231862783432007\n",
      "Step: 760000, Epsilon: 0.12759999999200577, Loss: 0.25983327627182007\n",
      "Step: 761000, Epsilon: 0.1271099999920139, Loss: 0.28626057505607605\n",
      "Step: 762000, Epsilon: 0.126619999992022, Loss: 0.3612942695617676\n",
      "Step: 763000, Epsilon: 0.12612999999203012, Loss: 0.28896838426589966\n",
      "Step: 764000, Epsilon: 0.12563999999203823, Loss: 0.33800387382507324\n",
      "Step: 765000, Epsilon: 0.12514999999204635, Loss: 0.28211677074432373\n",
      "Step: 766000, Epsilon: 0.12465999999204483, Loss: 0.2508818805217743\n",
      "Step: 767000, Epsilon: 0.12416999999203907, Loss: 0.3480883538722992\n",
      "Step: 768000, Epsilon: 0.1236799999920333, Loss: 0.2841961979866028\n",
      "Step: 769000, Epsilon: 0.12318999999202754, Loss: 0.34134575724601746\n",
      "Step: 770000, Epsilon: 0.12269999999202177, Loss: 0.3428354263305664\n",
      "Step: 771000, Epsilon: 0.12220999999201601, Loss: 0.29963186383247375\n",
      "Step: 772000, Epsilon: 0.12171999999201025, Loss: 0.31080007553100586\n",
      "Step: 773000, Epsilon: 0.12122999999200448, Loss: 0.34413769841194153\n",
      "Step: 774000, Epsilon: 0.12073999999199872, Loss: 0.3108513653278351\n",
      "Step: 775000, Epsilon: 0.12024999999199296, Loss: 0.3725651502609253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDQNTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/hanabi/src/agent/dqn/train.py:89\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Agent\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, config\u001b[38;5;241m.\u001b[39mdouble, config\u001b[38;5;241m.\u001b[39mtau, config\u001b[38;5;241m.\u001b[39mgamma, config\u001b[38;5;241m.\u001b[39mpolicy_update_freq, config\u001b[38;5;241m.\u001b[39mtarget_update_freq, config\u001b[38;5;241m.\u001b[39mstart_epsilon, config\u001b[38;5;241m.\u001b[39mend_epsilon, config\u001b[38;5;241m.\u001b[39mn_times, config\u001b[38;5;241m.\u001b[39msad, config\u001b[38;5;241m.\u001b[39mshuffle_observation, config\u001b[38;5;241m.\u001b[39mcheckpoint_freq, config\u001b[38;5;241m.\u001b[39mcheckpoint_dir, config\u001b[38;5;241m.\u001b[39mlog, config\u001b[38;5;241m.\u001b[39mlog_interval, config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     90\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_maker()\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mdone:\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/hanabi/src/env/play.py:54\u001b[0m, in \u001b[0;36mrun_episode_single_agent\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m     49\u001b[0m max_rewards[player] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(rewards[player], max_rewards[player])\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# save the results to the replay buffer\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# TODO: Episodic memory\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# agents[player].store(last_observations[player], last_actions[player], observation, reward, done)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_observations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# store the last observation and action\u001b[39;00m\n\u001b[1;32m     57\u001b[0m last_actions[player] \u001b[38;5;241m=\u001b[39m action\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/hanabi/src/agent/dqn/agent.py:155\u001b[0m, in \u001b[0;36mDQNAgent.step\u001b[0;34m(self, transition, training)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# add the transition to the buffer\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     state, action, reward, next_state, done \u001b[38;5;241m=\u001b[39m transition\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# decay the epsilon value\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_epsilon, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay)\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/hanabi/src/agent/dqn/agent.py:138\u001b[0m, in \u001b[0;36mDQNAgent._store_transition\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    132\u001b[0m next_state_tensor \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(next_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(next_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool),\n\u001b[1;32m    135\u001b[0m }\n\u001b[1;32m    136\u001b[0m done_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(done, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnext_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torchrl/data/replay_buffers/replay_buffers.py:540\u001b[0m, in \u001b[0;36mReplayBuffer.add\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39mndim), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torchrl/data/replay_buffers/replay_buffers.py:544\u001b[0m, in \u001b[0;36mReplayBuffer._add\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replay_lock:\n\u001b[0;32m--> 544\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39madd(index)\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torchrl/data/replay_buffers/writers.py:165\u001b[0m, in \u001b[0;36mRoundRobinWriter.add\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    163\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replicate_index(index)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39m_attached_entities:\n\u001b[0;32m--> 165\u001b[0m     \u001b[43ment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmark_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torchrl/data/replay_buffers/replay_buffers.py:678\u001b[0m, in \u001b[0;36mReplayBuffer.mark_update\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmark_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmark_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torchrl/data/replay_buffers/samplers.py:607\u001b[0m, in \u001b[0;36mPrioritizedSampler.mark_update\u001b[0;34m(self, index, storage)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmark_update\u001b[39m(\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28mself\u001b[39m, index: Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], \u001b[38;5;241m*\u001b[39m, storage: Storage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    606\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priority\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_priority\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torchrl/data/replay_buffers/samplers.py:584\u001b[0m, in \u001b[0;36mPrioritizedSampler.update_priority\u001b[0;34m(self, index, priority, storage)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m priority\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m    582\u001b[0m             priority \u001b[38;5;241m=\u001b[39m priority[valid_index]\n\u001b[0;32m--> 584\u001b[0m max_p, max_p_idx \u001b[38;5;241m=\u001b[39m \u001b[43mpriority\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m cur_max_priority, cur_max_priority_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_priority\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_max_priority \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_p \u001b[38;5;241m>\u001b[39m cur_max_priority:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "DQNTrainer(config).train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
