{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hanabi.agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanabi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainConfig\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m TrainConfig(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hanabi.agents'"
     ]
    }
   ],
   "source": [
    "from hanabi.agent.dqn.train import TrainConfig, DQNTrainer\n",
    "\n",
    "config = TrainConfig(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkivalm\u001b[0m (\u001b[33mkivalm-University of KwaZulu-Natal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/HonorsProject/code/wandb/run-20240829_014416-gi7h695i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/gi7h695i' target=\"_blank\">swept-butterfly-78</a></strong> to <a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors' target=\"_blank\">https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/gi7h695i' target=\"_blank\">https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/gi7h695i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kivalm-University%20of%20KwaZulu-Natal/Honors/runs/gi7h695i?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x709e3f751b20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Honors\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/HonorsProject/code/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1002: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000, Epsilon: 0.9901000000002149, Loss: 1.0384279489517212\n",
      "Step: 20000, Epsilon: 0.9802000000004298, Loss: 0.9269928932189941\n",
      "Step: 30000, Epsilon: 0.9703000000006448, Loss: 1.0391571521759033\n",
      "Step: 40000, Epsilon: 0.9604000000008597, Loss: 1.0378286838531494\n",
      "Step: 50000, Epsilon: 0.9505000000010746, Loss: 0.9808957576751709\n",
      "Step: 60000, Epsilon: 0.9406000000012895, Loss: 0.9166205525398254\n",
      "Step: 70000, Epsilon: 0.9307000000015044, Loss: 0.9235352277755737\n",
      "Step: 80000, Epsilon: 0.9208000000017194, Loss: 0.9330523014068604\n",
      "Step: 90000, Epsilon: 0.9109000000019343, Loss: 0.9142464399337769\n",
      "Step: 100000, Epsilon: 0.9010000000021492, Loss: 0.7494539618492126\n",
      "Step: 110000, Epsilon: 0.8911000000023641, Loss: 1.0330078601837158\n",
      "Step: 120000, Epsilon: 0.881200000002579, Loss: 1.0023536682128906\n",
      "Step: 130000, Epsilon: 0.871300000002794, Loss: 0.9503034353256226\n",
      "Step: 140000, Epsilon: 0.8614000000030089, Loss: 0.9306772947311401\n",
      "Step: 150000, Epsilon: 0.8515000000032238, Loss: 0.9403063058853149\n",
      "Step: 160000, Epsilon: 0.8416000000034387, Loss: 0.8626872301101685\n",
      "Step: 170000, Epsilon: 0.8317000000036536, Loss: 0.8818427920341492\n",
      "Step: 180000, Epsilon: 0.8218000000038685, Loss: 1.0061225891113281\n",
      "Step: 190000, Epsilon: 0.8119000000040835, Loss: 1.0550107955932617\n",
      "Step: 200000, Epsilon: 0.8020000000042984, Loss: 0.9008751511573792\n",
      "Step: 210000, Epsilon: 0.7921000000045133, Loss: 0.8015955686569214\n",
      "Step: 220000, Epsilon: 0.7822000000047282, Loss: 0.8606793880462646\n",
      "Step: 230000, Epsilon: 0.7723000000049431, Loss: 0.9155343174934387\n",
      "Step: 240000, Epsilon: 0.7624000000051581, Loss: 0.7900341153144836\n",
      "Step: 250000, Epsilon: 0.752500000005373, Loss: 0.808952271938324\n",
      "Step: 260000, Epsilon: 0.7426000000055879, Loss: 0.9295843839645386\n",
      "Step: 270000, Epsilon: 0.7327000000058028, Loss: 0.9441630244255066\n",
      "Step: 280000, Epsilon: 0.7228000000060177, Loss: 0.8426218628883362\n",
      "Step: 290000, Epsilon: 0.7129000000062327, Loss: 0.9196902513504028\n",
      "Step: 300000, Epsilon: 0.7030000000064476, Loss: 0.9320114254951477\n",
      "Step: 310000, Epsilon: 0.6931000000066625, Loss: 0.893084704875946\n",
      "Step: 320000, Epsilon: 0.6832000000068774, Loss: 0.84810471534729\n",
      "Step: 330000, Epsilon: 0.6733000000070923, Loss: 0.9154960513114929\n",
      "Step: 340000, Epsilon: 0.6634000000073073, Loss: 1.0161325931549072\n",
      "Step: 350000, Epsilon: 0.6535000000075222, Loss: 0.7860860824584961\n",
      "Step: 360000, Epsilon: 0.6436000000077371, Loss: 0.9594812393188477\n",
      "Step: 370000, Epsilon: 0.633700000007952, Loss: 0.9428806304931641\n",
      "Step: 380000, Epsilon: 0.6238000000081669, Loss: 0.9091376066207886\n",
      "Step: 390000, Epsilon: 0.6139000000083819, Loss: 0.9280447959899902\n",
      "Step: 400000, Epsilon: 0.6040000000085968, Loss: 0.8792089223861694\n",
      "Step: 410000, Epsilon: 0.5941000000088117, Loss: 0.821175754070282\n",
      "Step: 420000, Epsilon: 0.5842000000090266, Loss: 0.8580361008644104\n",
      "Step: 430000, Epsilon: 0.5743000000092415, Loss: 0.9167187809944153\n",
      "Step: 440000, Epsilon: 0.5644000000094564, Loss: 0.7265638113021851\n",
      "Step: 450000, Epsilon: 0.5545000000096714, Loss: 0.878629207611084\n",
      "Step: 460000, Epsilon: 0.5446000000098863, Loss: 0.8185449838638306\n",
      "Step: 470000, Epsilon: 0.5347000000101012, Loss: 0.9964932799339294\n",
      "Step: 480000, Epsilon: 0.5248000000103161, Loss: 0.8624277710914612\n",
      "Step: 490000, Epsilon: 0.514900000010531, Loss: 0.9755768775939941\n",
      "Step: 500000, Epsilon: 0.505000000010746, Loss: 0.7977108955383301\n",
      "Step: 510000, Epsilon: 0.4951000000109609, Loss: 0.771676778793335\n",
      "Step: 520000, Epsilon: 0.4852000000111758, Loss: 0.7886072397232056\n",
      "Step: 530000, Epsilon: 0.4753000000113907, Loss: 0.8684666156768799\n",
      "Step: 540000, Epsilon: 0.46540000001160564, Loss: 0.8252175450325012\n",
      "Step: 550000, Epsilon: 0.45550000001182056, Loss: 0.9004250764846802\n",
      "Step: 560000, Epsilon: 0.4456000000120355, Loss: 0.7593008279800415\n",
      "Step: 570000, Epsilon: 0.4357000000122504, Loss: 0.8257430195808411\n",
      "Step: 580000, Epsilon: 0.4258000000124653, Loss: 0.8392453789710999\n",
      "Step: 590000, Epsilon: 0.41590000001268024, Loss: 0.8629459142684937\n",
      "Step: 600000, Epsilon: 0.40600000001289516, Loss: 0.8833209276199341\n",
      "Step: 610000, Epsilon: 0.3961000000131101, Loss: 0.7828450798988342\n",
      "Step: 620000, Epsilon: 0.386200000013325, Loss: 0.7336264252662659\n",
      "Step: 630000, Epsilon: 0.3763000000135399, Loss: 0.7970765829086304\n",
      "Step: 640000, Epsilon: 0.36640000001375483, Loss: 0.7635889649391174\n",
      "Step: 650000, Epsilon: 0.35650000001396975, Loss: 0.7350642681121826\n",
      "Step: 660000, Epsilon: 0.3466000000141847, Loss: 0.8328114748001099\n",
      "Step: 670000, Epsilon: 0.3367000000143996, Loss: 0.7349026203155518\n",
      "Step: 680000, Epsilon: 0.3268000000146145, Loss: 0.7873451709747314\n",
      "Step: 690000, Epsilon: 0.31690000001482943, Loss: 0.8076246976852417\n",
      "Step: 700000, Epsilon: 0.30700000001504435, Loss: 0.8013268709182739\n",
      "Step: 710000, Epsilon: 0.29710000001525927, Loss: 0.7252265214920044\n",
      "Step: 720000, Epsilon: 0.2872000000154742, Loss: 0.6553447246551514\n",
      "Step: 730000, Epsilon: 0.2773000000156891, Loss: 0.8529142141342163\n",
      "Step: 740000, Epsilon: 0.267400000015904, Loss: 0.8474361896514893\n",
      "Step: 750000, Epsilon: 0.25750000001611895, Loss: 0.8265454173088074\n",
      "Step: 760000, Epsilon: 0.24760000001626656, Loss: 0.7474744319915771\n",
      "Step: 770000, Epsilon: 0.23770000001620392, Loss: 0.7779802083969116\n",
      "Step: 780000, Epsilon: 0.22780000001614129, Loss: 0.7500913739204407\n",
      "Step: 790000, Epsilon: 0.21790000001607865, Loss: 0.7357000112533569\n",
      "Step: 800000, Epsilon: 0.208000000016016, Loss: 0.7539363503456116\n",
      "Step: 810000, Epsilon: 0.19810000001595338, Loss: 0.7914316654205322\n",
      "Step: 820000, Epsilon: 0.18820000001589074, Loss: 0.7463850378990173\n",
      "Step: 830000, Epsilon: 0.1783000000158281, Loss: 0.7496234178543091\n",
      "Step: 840000, Epsilon: 0.16840000001576547, Loss: 0.7816998362541199\n",
      "Step: 850000, Epsilon: 0.15850000001570283, Loss: 0.7013827562332153\n",
      "Step: 860000, Epsilon: 0.1486000000156402, Loss: 0.6279006600379944\n",
      "Step: 870000, Epsilon: 0.13870000001557756, Loss: 0.6700106263160706\n",
      "Step: 880000, Epsilon: 0.12880000001551492, Loss: 0.5783388614654541\n",
      "Step: 890000, Epsilon: 0.11890000001545228, Loss: 0.792626142501831\n",
      "Step: 900000, Epsilon: 0.10900000001538965, Loss: 0.7773995995521545\n",
      "Step: 910000, Epsilon: 0.09910000001532701, Loss: 0.6592898368835449\n",
      "Step: 920000, Epsilon: 0.08920000001526437, Loss: 0.7826210856437683\n",
      "Step: 930000, Epsilon: 0.07930000001520174, Loss: 0.6338722705841064\n",
      "Step: 940000, Epsilon: 0.0694000000151391, Loss: 0.7200711965560913\n",
      "Step: 950000, Epsilon: 0.0595000000150975, Loss: 0.6366121768951416\n",
      "Step: 960000, Epsilon: 0.04960000001510425, Loss: 0.7336524724960327\n",
      "Step: 970000, Epsilon: 0.039700000015111, Loss: 0.6491807103157043\n",
      "Step: 980000, Epsilon: 0.029800000015117754, Loss: 0.7672337293624878\n",
      "Step: 990000, Epsilon: 0.019900000015124507, Loss: 0.5854321718215942\n",
      "Step: 1000000, Epsilon: 0.01000000001513126, Loss: 0.7369786500930786\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DQNTrainer(config).train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
